{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Anomaly Detection\n",
    "\n",
    "Author: Rithwik Gupta (rithwikca2020@gmail.com)\n",
    "\n",
    "Supervised by Daniel Muthukrishna (danmuth@mit.edu)\n",
    "\n",
    "This notebook generates the results in the paper \"A Classifier-Based Approach to Multi-Class Anomaly Detection for\n",
    "Astronomical Transients\" https://arxiv.org/abs/2403.14742\n",
    "\n",
    "In this notebook, we will preprocess and explores the dataset, and then train a recurrent neural network classifier on known classes. We will then uses the classifier as an encoder to generate a latent space for anomaly detection. We will train a Isolation Forests for each known class and apply \"Multi-Class Isoaltion Forests\" to identify anomalies in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOGRUcAwjInY",
    "outputId": "e78d7a88-aeae-4b42-f8d1-7b4a97823c56",
    "tags": []
   },
   "source": [
    "## Load the Data\n",
    "\n",
    "We will use simulated light curves matching the properties of the Zwicky Transient Facility. These simulations were developed using similar code used to generate the [PLAsTiCC](https://plasticc.org/) simulations. We ahve done some preprocessing on these light curves, and have will load the light curves that have been previously stored into pickle files.\n",
    "\n",
    "Download the data from here: https://drive.google.com/file/d/1rfc-fWhYgNgz7szmIi9Kv5xdcWvtCmdh/view?usp=drive_link (We have also provided the data on the Bridges-2 cluster)\n",
    "\n",
    "\n",
    "We use the following 5 classes as anomlaous classes: 'AGN', 'CaRT', 'KNe', 'PISN', 'ILOT', 'uLens-BSR'\n",
    "\n",
    "And the following 12 classes as the common classes: 'SNIa', 'SNIa-91bg', 'SNIax', 'SNIb', 'SNIc', 'SNIc-BL', 'SNII', 'SNIIn', 'SNIIb', 'TDE', 'SLSN-I',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names = ['lc_classnum_Ia.pickle',\n",
    "'lc_classnum_Ia-91bg.pickle', \n",
    "'lc_classnum_Iax.pickle', \n",
    "'lc_classnum_Ib.pickle', \n",
    "'lc_classnum_Ic.pickle',  \n",
    "'lc_classnum_Ic-BL.pickle',  \n",
    "'lc_classnum_II.pickle', \n",
    "'lc_classnum_IIn.pickle', \n",
    "'lc_classnum_IIb.pickle', \n",
    "'lc_classnum_TDE.pickle', \n",
    "'lc_classnum_SLSN-I.pickle', \n",
    "'lc_classnum_AGN.pickle',\n",
    "'lc_classnum_CART.pickle',\n",
    "'lc_classnum_Kilonova.pickle',\n",
    "'lc_classnum_PISN.pickle',\n",
    "'lc_classnum_ILOT.pickle',\n",
    "'lc_classnum_uLens-BSR.pickle']\n",
    "\n",
    "# Class names in the same order as the filenames\n",
    "classes = ['SNIa', 'SNIa-91bg', 'SNIax', 'SNIb', 'SNIc', 'SNIc-BL', 'SNII', 'SNIIn', 'SNIIb', 'TDE', 'SLSN-I', 'AGN', 'CaRT', 'KNe', 'PISN', 'ILOT', 'uLens-BSR']\n",
    "\n",
    "# Map class names to file names\n",
    "file_to_class = dict(zip(file_names, classes)) # Dictionary from filename to the classname\n",
    "class_to_file = {v: k for k, v in file_to_class.items()} # # Dictionary from classname to the filename\n",
    "\n",
    "# Define Anomalous Classes as the last 5 classes, and common classes as the first 12 classes\n",
    "anom_classes = classes[-5:]\n",
    "non_anom_classes = classes[:-5]\n",
    "\n",
    "# Colors for plotting  \n",
    "colors = ['r', 'g', 'y', 'b', 'purple', 'orange', 'gray', 'k', 'm', 'c', 'brown', 'olive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle files\n",
    "full_data = []\n",
    "for filename in file_names:\n",
    "  data = pickle.load(open(f\"ZTF_sims/{filename}\", \"rb\"))\n",
    "  full_data.append(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "Our ZTF simulations have already been processed such that all times are relative to the time of the first detection (trigger). Let's examine an example light curve in the dataset.\n",
    "\n",
    "The data has 5 columns: passband, time relative to trigger, flux, flux error, and photflag. \n",
    "\n",
    "photflag takes 3 values. 6144 for the first detection, 4096 for a detection, and 0 for a non-detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dataframe for an example PISN light curve with object ID 61_25609270.\n",
    "i = classes.index('PISN')\n",
    "full_data[i]['61_25609270']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot some example light curve\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axs = plt.subplots(3, 6, figsize=(25, 10))\n",
    "axes = axs.flatten()\n",
    "\n",
    "for i, c_name in enumerate(classes):\n",
    "\n",
    "    class_data = full_data[i]\n",
    "\n",
    "    axes[i].set_title(c_name)\n",
    "    # Plot one light curve from each class\n",
    "    for lc_name in class_data.keys():\n",
    "        lc_data = class_data[lc_name]\n",
    "        lc_data = pd.DataFrame(np.array(lc_data))\n",
    "        \n",
    "        # Check that the time zero corresponds to the first detection (phot flag 6144)\n",
    "        assert(lc_data[lc_data['photflag'] == 6144]['time'].all() == 0)\n",
    "\n",
    "        # Plot the light curve\n",
    "        pb_mask = lc_data['passband'] == 'g'\n",
    "        axes[i].errorbar(lc_data['time'][pb_mask], lc_data['flux'][pb_mask], yerr=lc_data['fluxErr'][pb_mask], fmt='o', color='tab:blue', alpha=0.5)\n",
    "        pb_mask = lc_data['passband'] == 'r'\n",
    "        axes[i].errorbar(lc_data['time'][pb_mask], lc_data['flux'][pb_mask], yerr=lc_data['fluxErr'][pb_mask], fmt='x', color='tab:orange', alpha=0.5)\n",
    "\n",
    "        break\n",
    "\n",
    "    axes[i].set_xlabel('Time (Days Since Trigger)')\n",
    "    axes[i].set_ylabel('Flux')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "kVWuwpVfjYJ1",
    "outputId": "1bcb0342-eb2c-4d49-ecd7-4bb7e1242577"
   },
   "outputs": [],
   "source": [
    "# Plot example transients from a given class\n",
    "\n",
    "def plot_raw(class_name, num_samples=5):\n",
    "    \n",
    "    class_ind = file_names.index(class_to_file[class_name])\n",
    "\n",
    "    keys = list(full_data[class_ind].keys())\n",
    "    key = None\n",
    "    counter = 0\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        if full_data[class_ind][key].meta['redshift'] < 0.2 and full_data[class_ind][key][np.argmax(full_data[class_ind][key]['flux'])]['time'] < 0:\n",
    "        \n",
    "            # print(full_data[class_ind][key].meta['class_num'])\n",
    "            full_data[class_ind][key] = full_data[class_ind][key][(full_data[class_ind][key]['time'] < 70) & (full_data[class_ind][key]['time'] > -30)]\n",
    "         \n",
    "            plt.errorbar(full_data[class_ind][key][full_data[class_ind][key]['passband'] == 'g']['time'], full_data[class_ind][key][full_data[class_ind][key]['passband'] == 'g']['flux'], yerr = full_data[class_ind][key][full_data[class_ind][key]['passband'] == 'g']['fluxErr'], fmt='.',label='g')\n",
    "            plt.errorbar(full_data[class_ind][key][full_data[class_ind][key]['passband'] == 'r']['time'], full_data[class_ind][key][full_data[class_ind][key]['passband'] == 'r']['flux'], yerr = full_data[class_ind][key][full_data[class_ind][key]['passband'] == 'r']['fluxErr'], fmt='.',label='r')\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Time Since Trigger\")\n",
    "            plt.ylabel(\"Measured Flux\")\n",
    "            plt.title(f\"Sample {class_name}\")\n",
    "            plt.show()\n",
    "\n",
    "            counter += 1\n",
    "            if counter > num_samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_raw('SNIa', num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "The preprocessing steps will create and save 3 arrays.\n",
    "\n",
    "1. *x_data*: information for each light curve. Length of each entry should be the number of observations in that light curve. Each timestep should have the scaled time, scaled flux, scaled flux error, median passband wavelength\n",
    "\n",
    "2. *host_galaxy_info*: Include any contextual information to this array. We will save 2 parameters: redshift and Milky Way extinction.\n",
    "\n",
    "3. *target*: target class name for each light curve.\n",
    "\n",
    "Other stuff you need (dataset specific) follows\n",
    "\n",
    "1. file_names: slightly misleading name but its basically np.unique(target)\n",
    "\n",
    "2. file_to_class: dictionary from what is stored in target for each class to what you want to show on plots\n",
    "\n",
    "2. anom_classes: list of values in target that are those of anomalous classes\n",
    "\n",
    "3. non_anom_classes: list of values in target that are those of common classes\n",
    "\n",
    "You can see an examples in the 2nd code block\n",
    "\n",
    "Here we estimate the mean so that we have a reasonable constant to divide all fluxes by. We can't scale per light curve as thats not possible in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eyngx6GBnk-z"
   },
   "outputs": [],
   "source": [
    "x_data = [] # info => for each time step, store time, median passband wavelength, flux, flux error\n",
    "host_galaxy_info = [] # 2 numbers per time step: redshift and Milky Way extinction\n",
    "target = [] # store target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "9EufAJ19jTbH",
    "outputId": "9593ae00-ed40-4497-9b05-83496d1fd6a1"
   },
   "outputs": [],
   "source": [
    "# ZTF band median wavelengths (in micrometers)\n",
    "band_medians = {'r' : 0.4827, 'g' : 0.6223}\n",
    "\n",
    "# Time window around trigger\n",
    "before = 30\n",
    "after = 70\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"Processing {class_name} {i+1}/{len(classes)}\")\n",
    "    data = full_data[i]\n",
    "    ids = list(data.keys())\n",
    "\n",
    "    for id in ids:\n",
    "        df = data[id]\n",
    "        df = pd.DataFrame(np.array(df)) # astropy table to pandas dataframe\n",
    "        \n",
    "        # Get host galaxy info redshift and Milky Way extinction\n",
    "        cur_meta = [data[id].meta['redshift'], data[id].meta['mwebv']]\n",
    "\n",
    "        # Scale flux - We choose to scale the fluxes by 500 to make the values more manageable\n",
    "        df['flux'] = (df['flux']) / 500\n",
    "        df['fluxErr'] = (df['fluxErr']) / 500\n",
    "\n",
    "        # Only keep observations within the time window\n",
    "        trigger_mjd = 0 # CHANGE THIS IF TRIGGER IS NOT AT 0\n",
    "        df = df[(df['time'] > trigger_mjd - before) & (df['time'] < trigger_mjd + after)] # Cut on time\n",
    "\n",
    "        # Remove (empty) sufficiently small light curves\n",
    "        if (len(df) == 0): \n",
    "            continue\n",
    "\n",
    "        # Correct for cosmological time dilation\n",
    "        df['time'] = df['time'] / (1 + data[id].meta['redshift'])\n",
    "\n",
    "        # Sort by time\n",
    "        df.sort_values(\"time\", inplace=True) \n",
    "        df.drop('photflag', axis=1, inplace=True)\n",
    "\n",
    "        # Scale times to be between 0 and 1\n",
    "        df.time = (df.time - (-before)) / (after + before)\n",
    "\n",
    "        # Map passband to median wavelength\n",
    "        df['passband'] = df['passband'].map(band_medians)\n",
    "\n",
    "        # Store data\n",
    "        x_data.append(np.array(df))\n",
    "        target.append(class_name)\n",
    "        host_galaxy_info.append(cur_meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjUbRQ5MnwD7",
    "outputId": "a60f9ffa-62b3-4793-9c1b-872968ae1966"
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "def save(save_path , obj):\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "os.makedirs(\"processed\", exist_ok=True)        \n",
    "save(\"processed/target.pkl\", target)\n",
    "save(\"processed/x_data.pkl\", x_data)\n",
    "save(\"processed/host_galaxy_info.pkl\", host_galaxy_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "id": "gPD_0u79jQrv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "target = load(\"processed/target.pkl\")\n",
    "x_data = load(\"processed/x_data.pkl\")\n",
    "host_galaxy_info = load(\"processed/host_galaxy_info.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're here, you should have preprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cuts number of objects to 13,000 transients max\n",
    "np.random.seed(42)\n",
    "\n",
    "target = np.array(target)\n",
    "\n",
    "for class_name, counts in zip(*np.unique(target, return_counts=True)):\n",
    "    # Remove the extra samples\n",
    "    if counts > 13000:\n",
    "        indices = np.where(target == class_name)[0]\n",
    "        indices = np.random.choice(indices, counts - 13000, replace=False, )\n",
    "        target = np.delete(target, indices)\n",
    "        x_data = np.delete(x_data, indices, axis=0)\n",
    "        host_galaxy_info = np.delete(host_galaxy_info, indices, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare arrays for Tensorflow\n",
    "\n",
    "Typical Neural Networks require a fixed lenght input. However, Recurrent Neural Networks are capable of handling variable lengths, making them especially well-suited to time-series. However,  special, we can have a variable length. To use Tensorflow, we still need to provide a fixed length, but we can later using a Tensorflow Masking Layer to tell the RNN to ignore the extra time-steps later on. \n",
    "\n",
    "Let's find the maximum number of timesteps for any light curve in our dataset, and pad all light curves to that length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "U1JI_xyWBsG8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max timesteps is: 656\n"
     ]
    }
   ],
   "source": [
    "# Get number of observations for all light curves and get max number of timesteps \n",
    "lengths = []\n",
    "for lc in x_data:\n",
    "    lengths.append(len(lc))\n",
    "\n",
    "ntimesteps = np.max(lengths)\n",
    "print(\"Max timesteps is:\", ntimesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "iSv-PFrol7am",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pad for TF masking layer\n",
    "for ind in range(len(x_data)):\n",
    "    x_data[ind] = np.pad(x_data[ind], ((0, ntimesteps - len(x_data[ind])), (0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "zwy3wjpPmFMq",
    "outputId": "fca81a1f-1aa6-4992-8304-97a3766fe2b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into anomalous and normal classes and x inputs and y targets\n",
    "\n",
    "y_data_anom = []\n",
    "y_data_norm = []\n",
    "x_data_norm = []\n",
    "x_data_anom = []\n",
    "host_gal_anom = []\n",
    "host_gal = []\n",
    "\n",
    "for i in range(len(target)):\n",
    "\n",
    "    if (target[i] in anom_classes):\n",
    "        x_data_anom.append(x_data[i])\n",
    "        y_data_anom.append(target[i])\n",
    "        host_gal_anom.append(host_galaxy_info[i])\n",
    "\n",
    "    else:\n",
    "        x_data_norm.append(x_data[i])\n",
    "        y_data_norm.append(target[i])\n",
    "        host_gal.append(host_galaxy_info[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "5Z2RcWxomLZa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-hot Encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "y_data_norm = enc.fit_transform(np.array(y_data_norm).reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "N5N3kmc5mOcT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train-validation-test split: 80% training, 10% validation, 10% test \n",
    "\n",
    "X_train, X_test, host_gal_train, host_gal_test, y_train, y_test = train_test_split(x_data_norm, host_gal, y_data_norm, random_state = 40, test_size = 0.1)\n",
    "X_train, X_val, host_gal_train, host_gal_val, y_train, y_val = train_test_split(X_train, host_gal_train, y_train, random_state = 40, test_size = 1/9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "Bisar4x4mRA7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get class_weights for imbalanced classes\n",
    "class_weights = {i : 0 for i in range(y_train.shape[1])}\n",
    "\n",
    "for value in y_train:\n",
    "  class_weights[np.argmax(value)]+=1\n",
    "\n",
    "for id in class_weights.keys():\n",
    "  class_weights[id] = len(y_train) / class_weights[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "host_gal_train = np.array(host_gal_train)\n",
    "host_gal_test = np.array(host_gal_test)\n",
    "host_gal_val = np.array(host_gal_val)\n",
    "\n",
    "y_train = np.squeeze(y_train)\n",
    "y_val = np.squeeze(y_val)\n",
    "y_test = np.squeeze(y_test)\n",
    "y_data_anom = np.squeeze(y_data_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = enc.transform(np.array(non_anom_classes).reshape(-1, 1))\n",
    "\n",
    "ordered_class_names = [-1 for i in range(len(non_anom_classes))]\n",
    "\n",
    "\n",
    "for ind, i in enumerate(dummy.todense()):\n",
    "    ordered_class_names[np.argmax(i)] = classes[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Table for Training Counts\n",
    "\n",
    "train_counts = {i : 0 for i in classes}\n",
    "\n",
    "for i in y_train:\n",
    "    train_counts[ordered_class_names[np.argmax(i)]] += 1\n",
    "    \n",
    "val_counts = {i : 0 for i in classes}\n",
    "\n",
    "for i in y_val:\n",
    "    val_counts[ordered_class_names[np.argmax(i)]] += 1\n",
    "    \n",
    "test_counts = {i : 0 for i in classes}\n",
    "\n",
    "for i in y_test:\n",
    "    test_counts[ordered_class_names[np.argmax(i)]] += 1\n",
    "\n",
    "for i in y_data_anom:\n",
    "    test_counts[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = {i: [] for i in classes}\n",
    "\n",
    "for i in train_counts.keys():\n",
    "    full_data[i].append(train_counts[i])\n",
    "    \n",
    "for i in val_counts.keys():\n",
    "    full_data[i].append(val_counts[i])\n",
    "    \n",
    "for i in test_counts.keys():\n",
    "    full_data[i].append(test_counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNIa & 9314 & 1131 & 1142 & 11587 \\\\\n",
      "\\hline\n",
      "SNIa-91bg & 10361 & 1318 & 1321 & 13000 \\\\\n",
      "\\hline\n",
      "SNIax & 10413 & 1248 & 1339 & 13000 \\\\\n",
      "\\hline\n",
      "SNIb & 4197 & 507 & 563 & 5267 \\\\\n",
      "\\hline\n",
      "SNIc & 1279 & 169 & 135 & 1583 \\\\\n",
      "\\hline\n",
      "SNIc-BL & 1157 & 124 & 142 & 1423 \\\\\n",
      "\\hline\n",
      "SNII & 10420 & 1279 & 1301 & 13000 \\\\\n",
      "\\hline\n",
      "SNIIn & 10323 & 1359 & 1318 & 13000 \\\\\n",
      "\\hline\n",
      "SNIIb & 9882 & 1233 & 1208 & 12323 \\\\\n",
      "\\hline\n",
      "TDE & 9078 & 1162 & 1114 & 11354 \\\\\n",
      "\\hline\n",
      "SLSN-I & 10285 & 1322 & 1273 & 12880 \\\\\n",
      "\\hline\n",
      "AGN & 8473 & 1046 & 1042 & 10561 \\\\\n",
      "\\hline\n",
      "CaRT & 0 & 0 & 10353 & 10353 \\\\\n",
      "\\hline\n",
      "KNe & 0 & 0 & 11166 & 11166 \\\\\n",
      "\\hline\n",
      "PISN & 0 & 0 & 10840 & 10840 \\\\\n",
      "\\hline\n",
      "ILOT & 0 & 0 & 11128 & 11128 \\\\\n",
      "\\hline\n",
      "uLens-BSR & 0 & 0 & 11244 & 11244 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "# Make Latex Table of counts for each training, validation, test, and all data (Table 1 in paper)\n",
    "for key, value in full_data.items():\n",
    "    print(f\"{key} & {value[0]} & {value[1]} & {value[2]} & {value[1] + value[0] + value[2]} \\\\\\\\\")\n",
    "    print(\"\\hline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "\n",
    "Here, we develop a GRU RNN architecture. You can change this model to suit your classification\n",
    "\n",
    "Function to make model. Here you'll need to design the architecture. You'll probably just need to change the input shape for the input layers (commented with CHANGE) depending on whether you use host galaxy information or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "id": "CwOkVsDUmWUv",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'binary_weighted_focal_crossentropy' from 'keras.backend' (/Users/danielmuthukrishna/miniforge3/lib/python3.9/site-packages/keras/backend.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [358], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, TimeDistributed, Dense, Masking, concatenate, GRU\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dist\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/api/_v2/keras/layers/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Keras layers API.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Layer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/api/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf. namespace.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], _module_wrapper\u001b[38;5;241m.\u001b[39mTFModuleWrapper):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/api/keras/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constraints\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/api/keras/backend/__init__.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binary_crossentropy\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binary_focal_crossentropy\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binary_weighted_focal_crossentropy\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast_to_floatx\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'binary_weighted_focal_crossentropy' from 'keras.backend' (/Users/danielmuthukrishna/miniforge3/lib/python3.9/site-packages/keras/backend.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, TimeDistributed, Dense, Masking, concatenate, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from math import dist\n",
    "\n",
    "def make_classifier_model(latent_size):\n",
    "\n",
    "    num_classes = len(class_weights)\n",
    "    n_features = 4\n",
    "\n",
    "    input_1 = Input((ntimesteps, n_features), name='lc')  # X.shape = (Nobjects, Ntimesteps, 4) CHANGE\n",
    "\n",
    "    masking_input1 = Masking(mask_value=0.)(input_1)\n",
    "\n",
    "    lstm1 = GRU(100, return_sequences=True, activation='tanh')(masking_input1)\n",
    "    lstm2 = GRU(100, return_sequences=False, activation='tanh')(lstm1)\n",
    "\n",
    "    dense1 = Dense(100, activation='tanh')(lstm2)\n",
    "\n",
    "    input_2 = Input(shape = (2, ), name='host') # CHANGE\n",
    "\n",
    "    dense2 = Dense(10)(input_2)\n",
    "\n",
    "    merge1 = concatenate([dense1, dense2])\n",
    "\n",
    "    dense3 = Dense(100, activation='relu')(merge1)\n",
    "\n",
    "    dense4 = Dense(latent_size, activation='relu', name='latent')(dense3)\n",
    "\n",
    "    output = Dense(num_classes, activation='softmax')(dense4)\n",
    "\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Model' from 'tensorflow.keras' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [360], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Object to store a trained model. Has MCIF built in.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrained_Model\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Model' from 'tensorflow.keras' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Object to store a trained model. Has MCIF built in.\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.ensemble import IsolationForest\n",
    "class Trained_Model:\n",
    "    def __init__(self, size, redshift):\n",
    "        \n",
    "        self.size = size\n",
    "        self.redshift = redshift\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_model(self, path):\n",
    "        ins = 'No' if not self.redshift else ''\n",
    "        self.model=keras.models.load_model(f'Models/{path}')\n",
    "        self.latent_model = Model(inputs=[self.model.get_layer('lc').input, self.model.get_layer('host').input], outputs=self.model.get_layer('latent').output)\n",
    "        \n",
    "        \n",
    "    def _train_latent(self):\n",
    "        print(self.host_gal_train.shape)\n",
    "        self.train_latent = self.latent_model.predict([self.X_train, self.host_gal_train])\n",
    "        \n",
    "    def _test_latent(self):\n",
    "        self.test_latent = self.latent_model.predict([self.X_test, self.host_gal_test])\n",
    "        \n",
    "    def _val_latent(self):\n",
    "        self.val_latent = self.latent_model.predict([self.X_val, self.host_gal_val])\n",
    "        \n",
    "    def _anom_latent(self):\n",
    "        self.anom_latent = self.latent_model.predict([self.X_anom, self.host_gal_anom])\n",
    "    \n",
    "    def initialize_mcif(self):\n",
    "\n",
    "        self.iso_forests = []\n",
    "        \n",
    "\n",
    "        full_latent_data = [[] for i in range(12)]\n",
    "\n",
    "        for i in range(len(self.train_latent)):\n",
    "            full_latent_data[np.argmax(y_train[i])].append(self.train_latent[i])\n",
    "            \n",
    "        for i in range(len(self.val_latent)):\n",
    "            full_latent_data[np.argmax(y_val[i])].append(self.val_latent[i])\n",
    "\n",
    "\n",
    "        for i in range(12):\n",
    "            self.iso_forests.append(IsolationForest(random_state=0, max_samples = 'auto', n_estimators=200).fit(np.array(full_latent_data[i])))\n",
    "\n",
    "        # self.iso_forests.append(iso_forests)\n",
    "        \n",
    "        \n",
    "    def score(self, x_data):\n",
    "        scores = [-det.decision_function(x_data) for det in self.iso_forests]\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        scores = scores.T\n",
    "        return [np.min(i) for i in scores]\n",
    "    \n",
    "    def score_val(self):\n",
    "        self.val_scores = self.score(self.val_latent)\n",
    "        \n",
    "    def score_test(self):\n",
    "        self.test_scores = self.score(self.test_latent)\n",
    "        \n",
    "    def score_anom(self):\n",
    "        self.anom_scores = self.score(self.anom_latent)\n",
    "        \n",
    "    def score_train(self):\n",
    "        self.train_scores = self.score(self.train_latent)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_classifier_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [361], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m latent_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m make_classifier_model(latent_size) \u001b[38;5;66;03m# HERE YOU CAN CHANGE THE LATENT SPACE SIZE\u001b[39;00m\n\u001b[1;32m      4\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m      5\u001b[0m                               patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      6\u001b[0m                               min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,                               \n\u001b[1;32m      7\u001b[0m                               monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                               restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                               )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_classifier_model' is not defined"
     ]
    }
   ],
   "source": [
    "latent_size=9\n",
    "model = make_classifier_model(latent_size) # HERE YOU CAN CHANGE THE LATENT SPACE SIZE\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "                              patience=5,\n",
    "                              min_delta=0.001,                               \n",
    "                              monitor=\"val_loss\",\n",
    "                              restore_best_weights=True\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    history = model.fit(x = [X_train, host_gal_train], validation_data=([X_val, host_gal_val], y_val), y = y_train, epochs=40, batch_size = 128, class_weight = class_weights, callbacks=[early_stopping])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "model.save(f\"Models/RedshiftLatent_{latent_size}\")\n",
    "save(f\"Models/RedshiftLatent_{latent_size}_history\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is loading a previous model. All this does is save you from having to initialize MCIF again, call .predict on the test data again, etc.\n",
    "# Don't run this the first time\n",
    "best = load(\"FINALFINAL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_data_anom = np.array(x_data_anom)\n",
    "host_gal_anom = np.array(host_gal_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = Trained_model(latent_size, True)\n",
    "# Load the model\n",
    "best.load_model(f\"RedshiftLatent_{latent_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a lot of scores\n",
    "\n",
    "best.train_latent = best.latent_model.predict([X_train, host_gal_train])\n",
    "best.val_latent = best.latent_model.predict([X_val, host_gal_val])\n",
    "best.initialize_mcif()\n",
    "best.anom_latent = best.latent_model.predict([x_data_anom, host_gal_anom])\n",
    "\n",
    "best.score_anom()\n",
    "best.score_val()\n",
    "\n",
    "best.test_predictions = best.model.predict([X_test, host_gal_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best.test_latent = best.latent_model.predict([X_test, host_gal_test])\n",
    "best.score_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_anom_order = [ordered_class_names.index(file_to_class[i]) for i in non_anom_classes]\n",
    "# This is the order of indices that we want to generate plots for\n",
    "# I.e. if the one-hot vector for SNIa has a 1 at position 9, then SNIa are actually the 9th class but we want it to be first. So non_anom_order[0] = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Generation\n",
    "\n",
    "All these functions take in predictions, and nothings stopping them from being pre-trigger predictions, predictions 30 days post-trigger, etc.\n",
    "To get the classifier's output for partial light curves, you'll need to create new X_test and x_data_anom and use the cut_curve function (defined a good bit below this block) to cut the light curve. You can see an example with x_data_pre 2 blocks below this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have fixed colors per class\n",
    "\n",
    "color_from_class = {c_name : colors[ordered_class_names.index(c_name)] for c_name in non_anom_classes}\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "anom_colors = prop_cycle.by_key()['color']\n",
    "\n",
    "\n",
    "for ind, c_name in enumerate(anom_classes):\n",
    "    color_from_class[c_name] = anom_colors[ind]\n",
    "    color_from_class[c_name] = anom_colors[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# Matplotlib set up\n",
    "font = {'size'   : 17}\n",
    "matplotlib.rc('font', **font)    \n",
    "colors = ['r', 'g', 'y', 'b', 'purple', 'orange', 'gray', 'k', 'm', 'c', 'brown', 'olive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_order(output): # Converts from file_name to position in order\n",
    "    return file_names.index(class_to_file[ordered_class_names[np.argmax(output)]])\n",
    "\n",
    "\n",
    "def saveplot(savepath): # Saves a plot (called in like every function)\n",
    "    if (savepath):\n",
    "        if (\"UMAP\" not in savepath):\n",
    "            plt.savefig(savepath + '.pdf', bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(savepath + '.png', bbox_inches='tight')\n",
    "\n",
    "def average_score(scores_maj, y_data_maj, scores_anom, y_data_anom, title=\"\", savepath=None):\n",
    "    '''\n",
    "    Calculate the average anomaly score for each class.\n",
    "    Input\n",
    "    Scores and targets for both anomalous and majority classes\n",
    "    '''\n",
    "    total_counts = {i : 0 for i in file_to_class.values()} # Change file_to_class.values() to a list of your classes\n",
    "    average_score = {i : 0 for i in file_to_class.values()}\n",
    "\n",
    "\n",
    "    for i in range(len(y_data_maj)):\n",
    "\n",
    "        total_counts[file_to_class[file_names[to_order(y_data_maj[i])]]] += 1\n",
    "        average_score[file_to_class[file_names[to_order(y_data_maj[i])]]] += scores_maj[i]\n",
    "\n",
    "    for i in range(len(y_data_anom)):\n",
    "        total_counts[file_to_class[y_data_anom[i]]] += 1\n",
    "        average_score[file_to_class[y_data_anom[i]]] += scores_anom[i]\n",
    "\n",
    "    for key in total_counts.keys():\n",
    "        if (total_counts[key] == 0):\n",
    "            continue\n",
    "        average_score[key] /= total_counts[key]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 13))\n",
    "    \n",
    "    averages = list(average_score.values())\n",
    "\n",
    "    cmap = matplotlib.cm.Blues(np.linspace(0,1,100))\n",
    "    cmap = matplotlib.colors.ListedColormap(cmap[25:75,:-1])\n",
    "\n",
    "    im = ax.imshow([averages], cmap=cmap)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(len(averages)), list(average_score.keys()), fontsize=15, rotation=45)\n",
    "    for x in range(len(averages)):\n",
    "      ax.annotate(str(round(averages[x], 2)), xy=(x, 0),\n",
    "                  horizontalalignment='center',\n",
    "                  verticalalignment='center', fontsize=15, fontweight = \"bold\" if (x > len(np.unique(y_data_maj))) else \"normal\") # Change the condition for something being anomalous\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    saveplot(savepath)\n",
    "\n",
    "def median_score(scores_maj, y_data_maj, scores_anom, y_data_anom, title=\"\", savepath=None): # Literally the same thing but just the median now\n",
    "    # total_counts = {i : 0 for i in file_to_class.values()}\n",
    "    score_dist = {i : [] for i in file_to_class.values()}\n",
    "\n",
    "\n",
    "    for i in range(len(y_data_maj)):\n",
    "\n",
    "        score_dist[file_to_class[file_names[to_order(y_data_maj[i])]]].append(scores_maj[i])\n",
    "\n",
    "    for i in range(len(y_data_anom)):\n",
    "        score_dist[file_to_class[y_data_anom[i]]].append(scores_anom[i])\n",
    "\n",
    "    for key in score_dist.keys():\n",
    "        score_dist[key] = np.median(score_dist[key])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 13))\n",
    "    \n",
    "    averages = list(score_dist.values())\n",
    "\n",
    "    cmap = matplotlib.cm.Blues(np.linspace(0,1,100))\n",
    "    cmap = matplotlib.colors.ListedColormap(cmap[25:75,:-1])\n",
    "\n",
    "    im = ax.imshow([averages], cmap=cmap)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(range(len(averages)), list(score_dist.keys()), fontsize=15, rotation=45)\n",
    "    for x in range(len(averages)):\n",
    "      ax.annotate(str(round(averages[x], 2)), xy=(x, 0),\n",
    "                  horizontalalignment='center',\n",
    "                  verticalalignment='center', fontsize=15, fontweight = \"bold\" if (x > 11) else \"normal\")\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    saveplot(savepath)\n",
    "\n",
    "    \n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(predictions, labels, savepath=None): # Confusion Matrix plot, predictions is a list of probabilities\n",
    "    \n",
    "    single_pred = np.array([to_order(i) for i in predictions])\n",
    "    single_test = np.array([to_order(i) for i in labels])\n",
    "\n",
    "    cm = confusion_matrix(single_test, single_pred, labels = range(len(non_anom_classes)), normalize='true')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=np.round(cm, 2), display_labels=[file_to_class[i] for i in non_anom_classes]) # CHANGE DISPLAY LABELS\n",
    "\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "    fig = disp.figure_\n",
    "\n",
    "\n",
    "    disp.im_.colorbar.remove()\n",
    "\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(10)\n",
    "\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.xlabel(\"True Class\", fontsize=29)\n",
    "    plt.ylabel(\"Predicted Class\", fontsize=29)\n",
    "    # plt.title(\"Confusion Matrix on Full Time Series\", fontsize=27)\n",
    "    \n",
    "    saveplot(savepath)\n",
    "\n",
    "    \n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve, roc_auc_score, auc\n",
    "\n",
    "\n",
    "def plot_roc_curve(predictions, labels, savepath=None): # Same as confusion matrix\n",
    "    \n",
    "\n",
    "    single_test = np.array([np.argmax(i) for i in labels])\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in non_anom_order:\n",
    "      fpr, tpr, _ = roc_curve(y_true = single_test, y_score = predictions[:, i], pos_label = i)\n",
    "      plt.plot(fpr, tpr, label = ordered_class_names[i] + f\" ({round(auc(fpr, tpr), 2)})\", color=colors[i])\n",
    "\n",
    "    plt.legend(loc=0, fontsize=21)\n",
    "\n",
    "    # plt.title(\"ROC Curve for Pre-Trigger Light Curve\", fontsize=28)\n",
    "    plt.xlabel(\"True Positive Rate\", fontsize=29)\n",
    "    plt.ylabel(\"False Positive Rate\", fontsize=29)\n",
    "\n",
    "    plt.xticks(fontsize=24)\n",
    "    plt.yticks(fontsize=24)\n",
    "    \n",
    "    \n",
    "    saveplot(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [389], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# average_score(best.test_scores, y_val, best.anom_scores, y_data_anom, title=f\"Median Anomaly Score\\n(MCIF)\", savepath=f'last/MedianScoreRedo')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m median_score(best\u001b[38;5;241m.\u001b[39mtest_scores, y_val, best\u001b[38;5;241m.\u001b[39manom_scores, y_data_anom, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedian Anomaly Score\u001b[39m\u001b[38;5;124m\"\u001b[39m, savepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast/MedianScore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "# average_score(best.test_scores, y_val, best.anom_scores, y_data_anom, title=f\"Median Anomaly Score\\n(MCIF)\", savepath=f'last/MedianScoreRedo')\n",
    "median_score(best.test_scores, y_val, best.anom_scores, y_data_anom, title=f\"Median Anomaly Score\", savepath=f'last/MedianScore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# matplotlib.style.set('defualt')\n",
    "plot_roc_curve(best.test_predictions, y_test, savepath='last/ROCCurvePreNoTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 17}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "plot_confusion_matrix(best.test_predictions, y_test, savepath='last/ConfusionMatrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the precision-recall curve, calculate precision and recall as defined in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "def get_pr(minority, majority): # takes minority and majority data and gets the precision-recall with minority being True Positive and majority being False Positive\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    thresholds = []\n",
    "    minority=np.array(minority)\n",
    "    majority=np.array(majority)\n",
    "    for threshold in np.arange(min(np.min(minority),np.min(majority)), max(np.max(minority),np.max(majority)) + 0.002, 0.001):\n",
    "        ta = np.count_nonzero(minority > threshold)\n",
    "        fa = np.count_nonzero(majority > threshold)\n",
    "        tn = np.count_nonzero(majority < threshold)\n",
    "        fn = np.count_nonzero(minority < threshold)\n",
    "        if fn == 0 or fa == 0:\n",
    "          continue\n",
    "        if (ta + fa == 0):\n",
    "            continue\n",
    "        recall.append(ta / (ta + fn))\n",
    "        precision.append(ta / (ta + fa))\n",
    "        thresholds.append(threshold)\n",
    "    return thresholds, precision, recall\n",
    "\n",
    "def get_pr_auc(scores_maj, y_data_maj, scores_anom, y_data_anom): # Gets PR AUC, not actually used anywhere\n",
    "    np.random.seed(60)\n",
    "    \n",
    "    aucs = {}\n",
    "    \n",
    "    for ind, i in enumerate(non_anom_classes):\n",
    "        \n",
    "        mult_iso_maj_class = [scores_maj[t] for t in range(len(scores_maj)) if non_anom_classes[np.argmax(y_data_maj[t])] == i]\n",
    "\n",
    "        mult_iso_min_cur = np.random.choice(scores_anom, int(len(mult_iso_maj_class)), p=p_anom)\n",
    "\n",
    "        thresholds, precision, recall = get_pr(mult_iso_maj_class, mult_iso_min_cur)\n",
    "\n",
    "        aucs[file_to_class[i]] = auc(recall, precision)\n",
    "\n",
    "\n",
    "    for ind, i in enumerate(anom_classes):\n",
    "        mult_iso_min_class = [scores_anom[t] for t in range(len(y_data_anom)) if y_data_anom[t] == i] # all anomalies of current class (i)\n",
    "\n",
    "        mult_iso_min_class = np.random.choice(mult_iso_min_class, int(min(len(scores_maj), len(mult_iso_min_class))))\n",
    "\n",
    "        thresholds, precision, recall = get_pr(mult_iso_min_class, scores_maj)\n",
    "\n",
    "        aucs[file_to_class[i]] = auc(recall, precision)\n",
    "\n",
    "        \n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: RANDOM SUBSAMPLES\n",
    "\n",
    "Because a lot of the reported results rely on sampling a random subsample, I created the arrays p_norm and p_anom. These *rig* the probability of each transient being chosen randomly so that transients represented less frequently have a higher probability. This ensures an evenly sampled distribution. E.g. if we have 100 SNIa and 200 SNIIb in our majority sample, I still want the probability of choosing each to be the same.\n",
    "\n",
    "However you might want an uneven population distribution, in which case you can make p_anom and p_norm have all the same value or remove them when they are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CaRT': 10353, 'KNe': 11166, 'PISN': 10840, 'ILOT': 11128, 'uLens-BSR': 11244}\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "class_weights_anom = {i: 0 for i in anom_classes}\n",
    "\n",
    "for i in y_data_anom:\n",
    "  class_weights_anom[i]+=1\n",
    "\n",
    "print(class_weights_anom)\n",
    "for i in list(class_weights_anom.keys()):\n",
    "  class_weights_anom[i] = len(y_data_anom) / class_weights_anom[i]\n",
    "\n",
    "\n",
    "p_anom = [class_weights_anom[i] for i in y_data_anom]\n",
    "\n",
    "sum = np.sum(p_anom)\n",
    "\n",
    "for i in range(len(p_anom)):\n",
    "  p_anom[i] /= sum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function plots the precision recall curve\n",
    "def plot_pr_anom(scores_maj, y_data_maj, scores_anom, y_data_anom, s=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = gridspec.GridSpec(2, 3, width_ratios=[1,2,0], height_ratios=[1,1], wspace=0.3)\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[1, 0], sharex=ax3)\n",
    "    ax = fig.add_subplot(gs[:, 1:3])\n",
    "\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    \n",
    "    plt.ylim((0, 1))\n",
    "    plt.xlim((0, 1))\n",
    "    l = 3 # Linewidth\n",
    "    \n",
    "    for ind, i in enumerate(anom_classes):\n",
    "        mult_iso_min_class = [scores_anom[t] for t in range(len(scores_anom)) if y_data_anom[t] == i] # all anomalies of current class (i)\n",
    "\n",
    "        mult_iso_min_class = np.random.choice(mult_iso_min_class, int(min(len(scores_maj), len(mult_iso_min_class)))) # Subset of these anomalies because we have less testing data\n",
    "\n",
    "        thresholds, precision, recall = get_pr(mult_iso_min_class, scores_maj)\n",
    "        ax.plot(recall, precision, label=f\"{file_to_class[i]} ({round(auc(recall, precision), 2)})\", c = color_from_class[i], linewidth=l)\n",
    "\n",
    "        ax2.plot(thresholds, recall, label=f\"{file_to_class[i]}\", c = color_from_class[i], linewidth=l)\n",
    "\n",
    "        ax3.plot(thresholds, precision, label=f\"{file_to_class[i]}\", c = color_from_class[i], linewidth=l)\n",
    "\n",
    "    # Majority classes, defined below\n",
    "\n",
    "    thresholds, precision, recall = plot_pr_maj(scores_maj, y_data_maj, scores_anom, y_data_anom)\n",
    "\n",
    "    ax.plot(recall, precision, label=f\"Common Classes ({round(auc(recall, precision), 2)})\", c = 'grey', linewidth=l)\n",
    "\n",
    "    ax2.plot(thresholds, recall, label=f\"Common Classes\", c = 'grey', linewidth=l)\n",
    "\n",
    "    ax3.plot(thresholds, precision, label=f\"Common Classes\", c = 'grey', linewidth=l)\n",
    "\n",
    "\n",
    "    ax.set_ylabel(\"Precision\", fontsize=title_sz)\n",
    "    ax.set_xlabel(\"Recall\", fontsize=title_sz)\n",
    "    ax.legend(fontsize=22)\n",
    "\n",
    "    ax2.set_ylabel(\"Recall\", fontsize=title_sz)\n",
    "    ax2.set_xlabel(\"Threshold\", fontsize=title_sz)\n",
    "    # ax2.legend(fontsize=15)\n",
    "\n",
    "    ax3.set_ylabel(\"Precision\", fontsize=title_sz)\n",
    "    ax3.set_xlabel(\"Threshold\", fontsize=title_sz)\n",
    "    # ax3.legend(fontsize=15)\n",
    "\n",
    "    fig.suptitle(\"Anomaly Precision-Recall Curve\", fontsize=33)\n",
    "    ax.set_xticks(np.arange(0.2, 1.2, 0.2))\n",
    "\n",
    "\n",
    "    saveplot(s)\n",
    "\n",
    "def plot_pr_maj(scores_maj, y_data_maj, scores_anom, y_data_anom, s=None): # Returns precision/recall for majority classes. Not plotted\n",
    "\n",
    "    \n",
    "    mult_iso_maj_class = [scores_maj[t] for t in range(len(scores_maj))]\n",
    "    mult_iso_maj_class = np.random.choice(mult_iso_maj_class, 10000, p=p_norm)\n",
    "\n",
    "    mult_iso_min_cur = np.random.choice(scores_anom, int(len(mult_iso_maj_class)), p=p_anom)\n",
    "\n",
    "    thresholds, precision, recall = get_pr(mult_iso_maj_class, mult_iso_min_cur)\n",
    "\n",
    "\n",
    "    return thresholds, precision, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [391], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_pr_anom(best\u001b[38;5;241m.\u001b[39mtest_scores, y_test, best\u001b[38;5;241m.\u001b[39manom_scores, y_data_anom, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast/PRAnom\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pr_anom(best.test_scores, y_test, best.anom_scores, y_data_anom, s=\"last/PRAnom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Anomaly Scores\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "file_to_index = {x: list(file_to_class.values()).index(x) for x in file_to_class.values()}\n",
    "def plot_dist(scores_maj, y_data_maj, scores_anom, y_data_anom, savepath=None): # Same input as average/median_score\n",
    "    color = ['#ADD8E6'] * 12 + ['#FF6645'] * 5\n",
    "    \n",
    "    x=[]\n",
    "    g=[]\n",
    "\n",
    "    for i in range(len(scores_maj)):\n",
    "        g.append(ordered_class_names[np.argmax(y_data_maj[i])])\n",
    "        x.append(scores_maj[i])\n",
    "\n",
    "    for i in range(len(scores_anom)):\n",
    "        g.append(file_to_class[y_data_anom[i]])\n",
    "        x.append(scores_anom[i])\n",
    "\n",
    "    df = pd.DataFrame(dict(x=x, g=g))\n",
    "\n",
    "    df.sort_values('g', inplace=True, key=lambda x: x.map(file_to_index))\n",
    "\n",
    "    \n",
    "\n",
    "    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "    # Initialize the FacetGrid object\n",
    "    g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=15, height=.5, palette=color)\n",
    "\n",
    "    # Draw the densities in a few steps\n",
    "    g.map(sns.kdeplot, \"x\",\n",
    "          bw_adjust=.5, clip_on=False,\n",
    "          fill=True, alpha=1, linewidth=1.5)\n",
    "    g.map(sns.kdeplot, \"x\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n",
    "\n",
    "    # passing color=None to refline() uses the hue mapping, but we do color = 'blue'\n",
    "    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "\n",
    "    # Define and use a simple function to label the plot in axes coordinates\n",
    "    def label(x, color, label):\n",
    "        ax = plt.gca()\n",
    "        ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "    g.map(label, \"x\")\n",
    "\n",
    "    # Set the subplots to overlap\n",
    "    g.figure.subplots_adjust(hspace=-.25)\n",
    "\n",
    "    # Remove axes details that don't play well with overlap\n",
    "    g.set_titles(\"\")\n",
    "    g.set(yticks=[], ylabel=\"\")\n",
    "    g.set(xlabel=\"Anomaly Score\")\n",
    "\n",
    "    g.despine(bottom=True, left=True)\n",
    "    g.fig.suptitle(title, fontsize=23)\n",
    "\n",
    "    if (savepath):\n",
    "        g.figure.savefig(f\"{savepath}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [393], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_dist(best\u001b[38;5;241m.\u001b[39mtest_scores, y_val, best\u001b[38;5;241m.\u001b[39manom_scores, y_data_anom, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, savepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast/Distribution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "plot_dist(best.test_scores, y_val, best.anom_scores, y_data_anom, title=f\"\", savepath=f'last/Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly create subsamples and plot\n",
    "all_counts = {}\n",
    "from copy import deepcopy\n",
    "def plot_recall(majority, minority, s=None, seed=100): # USE DEFAULT SEED! :)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    k = 50\n",
    "\n",
    "    pref_counts = {}\n",
    "    pref_recall = {}\n",
    "    freqs = {}\n",
    "    for key in ['All'] + anom_classes:\n",
    "        pref_counts[key] = []\n",
    "        pref_recall[key] = []\n",
    "        freqs[key] = []\n",
    "\n",
    "    for i in ordered_class_names:\n",
    "        freqs[i] = []\n",
    "    \n",
    "    \n",
    "    rigged_p_anom = p_anom.copy()\n",
    "\n",
    "    # Uncomment to remove CaRTs\n",
    "\n",
    "    # for i in range(len(p_anom)):\n",
    "    #     if y_data_anom[i] == class_to_file['CaRT']:\n",
    "    #         rigged_p_anom[i] = 0\n",
    "\n",
    "    # sm = np.sum(rigged_p_anom)\n",
    "    # for i in range(len(rigged_p_anom)):\n",
    "    #     rigged_p_anom[i] /= sm\n",
    "\n",
    "\n",
    "    \n",
    "    for _ in range(k):\n",
    "\n",
    "      freq = {i : 0 for i in pref_counts.keys()}\n",
    "      for i in ordered_class_names:\n",
    "          freq[i] = 0\n",
    "\n",
    "      random_sample_maj = majority\n",
    "   \n",
    "    \n",
    "      min_mask = np.random.choice(range(len(minority)), int(len(majority)/220), p = rigged_p_anom) # Change /220 for different ratio\n",
    "      random_sample_min = np.array(minority)[min_mask]\n",
    "      min_classes = np.array(y_data_anom)[min_mask]\n",
    "        \n",
    "      # print(len(random_sample_maj), len(random_sample_min))\n",
    "\n",
    "      full_sample = [(i, ordered_class_names[np.argmax(y_test[ind])]) for ind, i in enumerate(random_sample_maj)] + [(i, min_classes[ind]) for ind, i in enumerate((random_sample_min))]\n",
    "      full_sample = list(reversed(sorted(full_sample)))\n",
    "\n",
    "      pref_sample = {i : [0] for i in anom_classes}\n",
    "      pref_sample['All'] = [0]\n",
    "\n",
    "     \n",
    "      for i in full_sample:\n",
    "        freq['All'] += 1\n",
    "        pref_sample['All'].append(pref_sample['All'][-1] + (i[1] in anom_classes))\n",
    "        \n",
    "        for key in anom_classes:\n",
    "           if (i[1] == key):\n",
    "               freq[key] += 1\n",
    "           pref_sample[key].append(pref_sample[key][-1] + (i[1] == key))\n",
    "\n",
    "        for key in ordered_class_names:\n",
    "            if (i[1] == key):\n",
    "               freq[key] += 1\n",
    "\n",
    "      prefr_sample = deepcopy(pref_sample)\n",
    "      for i in pref_sample.keys():\n",
    "          prefr_sample[i] = np.array(pref_sample[i]) / freq[i]\n",
    "          # assert(np.max(prefr_sample[i]) <= 1)\n",
    "\n",
    "        \n",
    "      for key in pref_sample.keys():\n",
    "          pref_counts[key].append(pref_sample[key])\n",
    "          pref_recall[key].append(prefr_sample[key])\n",
    "          freqs[key].append(freq[key])\n",
    "\n",
    "      for key in ordered_class_names:\n",
    "          freqs[key].append(freq[key])\n",
    "          \n",
    "\n",
    "    for i in freqs.keys():\n",
    "        print(i, np.mean(freqs[i]), np.std(freqs[i]))\n",
    "\n",
    "        all_counts[i]=freqs[i]\n",
    "    # return\n",
    "            \n",
    "    final = []\n",
    "            \n",
    "    for key, value in pref_counts.items():\n",
    "      med = []\n",
    "      dev = []\n",
    "\n",
    "      medr = []\n",
    "      devr = []\n",
    "        \n",
    "        \n",
    "      pref_counts = np.array(value).T\n",
    "      rec_here = np.array(pref_recall[key]).T\n",
    "\n",
    "      for i in range(len(pref_counts)):\n",
    "        pref_counts[i] = np.array(pref_counts[i])\n",
    "        rec_here[i] = np.array(rec_here[i])\n",
    "\n",
    "        # assert(np.max(rec_here[i]) <= 1)\n",
    "        # if (key == 'All'):\n",
    "        #     print(i, pref_sample[key][i])\n",
    "        med.append(np.mean(pref_counts[i]))\n",
    "        dev.append(np.std(pref_counts[i]))\n",
    "\n",
    "        medr.append(np.mean(rec_here[i]))\n",
    "        devr.append(np.std(rec_here[i]))\n",
    "\n",
    "\n",
    "      \n",
    "      med=np.array(med)\n",
    "      dev = np.array(dev)\n",
    "\n",
    "      medr=np.array(medr)\n",
    "      devr = np.array(devr)\n",
    "\n",
    "      # print(np.max(medr + devr))\n",
    "      \n",
    "      final.append([med, dev, key, medr, devr])\n",
    "\n",
    "    \n",
    "    font = {'size'   : 17}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    \n",
    "\n",
    "    ax.set_xlim(0, 2000)\n",
    "    ax.set_xlabel(\"Index (Top 2000 Scores)\", fontsize=18)\n",
    "    ax.set_ylabel(\"Recall\", fontsize=18)\n",
    "\n",
    "    ax.set_title(\"Anomalies Detected by Index\", fontsize=21)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    ax2.set_ylabel('Detected Anomalies', fontsize=16)\n",
    "    \n",
    "    ind = 0\n",
    "\n",
    "    # Comment if you don't want guessing line\n",
    "    x = np.array(range(0,2000))\n",
    "    y = 1/220 * x\n",
    "    plt.plot(x, y, label='Guessing', linestyle='dashed', color='grey')\n",
    "\n",
    "   \n",
    "    \n",
    "    for med, dev, label, medr, devr in final:\n",
    "        if (label != 'All'): # running this loop through with 'All' plots result for all classes\n",
    "            \n",
    "            continue\n",
    " \n",
    "        if label == 'All'):\n",
    "            ax.plot(medr, label=file_to_class[label] if label != 'All' else 'This Work', color=color_from_class[file_to_class[label]])\n",
    "            ax.fill_between(range(1, len(med) + 1), medr + devr, medr-devr, alpha = 0.2, color=color_from_class[file_to_class[label]])\n",
    "        else:\n",
    "            ax2.plot(range(1, len(med) + 1), med, label=file_to_class[label] if label != 'All' else 'This Work', color = u'#ff7f0e') # color=color_from_class[file_to_class[label]])\n",
    "            ax2.fill_between(range(1, len(med) + 1), med + dev, med-dev, alpha = 0.2, color = u'#ff7f0e')# , color=color_from_class[file_to_class[label]])\n",
    "\n",
    "    \n",
    "    \n",
    "    ax2.set_ylim(-0.1 * int(len(majority) / 220), 1.1 * int(len(majority) / 220))\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    \n",
    "    ax.tick_params(axis='y')\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    saveplot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plot_recall(best.test_scores, best.anom_scores, s='last/AnomByInd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i_ in anom_classes:\n",
    "    # i=file_to_class[i_]\n",
    "    i=i_\n",
    "    print(i, int(np.mean(all_counts[i])), \"\\pm\", int(np.std(all_counts[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size=9\n",
    "def plot_history(filepath, s= None):\n",
    "    hist = load(filepath) \n",
    "    \n",
    "    plt.plot(range(1, len(hist.history['val_accuracy']) + 1), hist.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(range(1, len(hist.history['accuracy']) + 1), np.array(hist.history['accuracy']), label='Training Accuracy') #  * len(X_val) /  len(X_train)\n",
    "    \n",
    "    plt.xlabel(\"Epoch\", fontsize=17)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=17)\n",
    "    \n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=14)\n",
    "    \n",
    "    # plt.title('Training/Validation Accuracy', fontsize=19)\n",
    "    saveplot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matplotlib.style.use('default')\n",
    "\n",
    "plot_history(f'Models/RedshiftLatent_{latent_size}_historyFINALFINAL', s = 'last/AccuracyHistory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UMAP reducer and plot UMAP\n",
    "\n",
    "\n",
    "def get_reducer(data): # Basic function\n",
    "    reducer = umap.UMAP(random_state=5, min_dist=0.5, n_neighbors=500)\n",
    "\n",
    "    reducer.fit(data)\n",
    "    \n",
    "    embedding=reducer.transform(data)\n",
    "    \n",
    "    return reducer, embedding\n",
    "    \n",
    "def plot_umap(embedding, labels, legend, uni=None, title='', savepath='', color=color): # This is very specific to my dataset, I'd reccomending writing your own code if you want to see the UMAP reduction\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    if uni == None:\n",
    "        uni = np.unique(labels)\n",
    "\n",
    "    print(uni)\n",
    "    for cc, i in enumerate(uni):\n",
    "        \n",
    "        e=[]\n",
    "        for ind, point in enumerate(labels):\n",
    "            # print(point)\n",
    "            if point == i:\n",
    "                e.append(embedding[ind])\n",
    "        \n",
    "        assert(not len(e) == 0)\n",
    "        e = np.array(e)\n",
    "        if (type(i) == int):\n",
    "            plt.scatter(e[:, 0], e[:, 1], label=file_to_class[non_anom_classes[i]], c=color_from_class[file_to_class[non_anom_classes[i]]]) # if cc != 0 else 'grey'\n",
    "        elif (i in anom_classes):\n",
    "            plt.scatter(e[:, 0], e[:, 1], label=file_to_class[i], c=color_from_class[i]) # if cc != 0 else 'grey'\n",
    "        else:\n",
    "            plt.scatter(e[:, 0], e[:, 1], label=i, c=\"Grey\") # if cc != 0 else 'grey'\n",
    "\n",
    "        \n",
    "        \n",
    "    plt.title(title, fontsize=27)\n",
    "    # plt.xlabel('Arbritary Units', fontsize=17)\n",
    "    # plt.ylabel('Arbritary Units', fontsize=17)\n",
    "    plt.xticks(range(-5, 25, 5))\n",
    "    plt.yticks(range(-5, 25, 5))\n",
    "    \n",
    "    plt.xlabel('UMAP 1', fontsize=17)\n",
    "    plt.ylabel('UMAP 2', fontsize=17)\n",
    "    plt.legend(fontsize=17,loc=\"upper right\")\n",
    "    saveplot(savepath)\n",
    "    \n",
    "    # return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "best.te_reducer, best.te_embedding = get_reducer(best.test_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.tr_reducer, best.tr_embedding = get_reducer(best.train_latent)\n",
    "best.a_reducer, best.a_embedding = get_reducer(best.anom_latent)\n",
    "# best.te_reducer, best.te_embedding = get_reducer(best.test_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save(\"FINALFINAL\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best = load(\"FINALFINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_umap(best.tr_embedding, [np.argmax(i) for i in y_train], [file_to_class[i] for i in non_anom_classes], uni = [ordered_class_names.index(file_to_class[i]) for i in non_anom_classes], title = \"UMAP Reduction on Training Data\", savepath=\"last/UMAPTrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(best.a_embedding, y_data_anom, [file_to_class[i] for i in reversed(np.unique(y_data_anom))], title=\"UMAP Reduction on Anomalous Data\", savepath=\"last/UMAPAnom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(best.te_embedding, [np.argmax(i) for i in y_test], ordered_class_names, uni = [ordered_class_names.index(file_to_class[i]) for i in non_anom_classes], title=\"UMAP Reduction on Testing Data\", savepath=\"last/UMAPTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(len(best.a_embedding)), round(len(best.te_embedding)/5), replace=False)\n",
    "vals = best.a_embedding[idx]\n",
    "labels = np.array(y_data_anom)[idx]\n",
    "\n",
    "plot_umap(np.append(best.te_embedding, vals, axis=0), ['Common Transients' for i in y_test] + list(labels), ['Common Transients'] + [file_to_class[i] for i in np.unique(labels)], title=\"UMAP Reduction on All Data\", savepath=\"last/UMAPAll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save(\"FinalTrainedModel\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_curve(lc, cutoff, ind=False): # assumes scaled cutoff and uses binary search\n",
    "  # print(lc.shape)\n",
    "  lo = 0\n",
    "  hi = len(lc)-1\n",
    "\n",
    "  while (lo < hi):\n",
    "    m = (lo + hi)//2\n",
    "    # print(lc[m][1], cutoff)\n",
    "    if (lc[m][1] > cutoff or not np.any(lc[m][1])):\n",
    "      hi = m\n",
    "    else:\n",
    "      lo = m+1\n",
    "\n",
    "  # print(lo)\n",
    "  \n",
    "  if (not ind):\n",
    "      for i in range(hi, len(lc)):\n",
    "        if (not np.any(lc[i])):\n",
    "          break\n",
    "    \n",
    "        lc[i] = np.zeros(4)\n",
    "\n",
    "  if (not ind):\n",
    "    return lc\n",
    "  else:\n",
    "    if (not np.any(lc[hi])):\n",
    "        hi-=1\n",
    "    return hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Real Time Results\n",
    "\n",
    "# Redefine p_anom and p_norm\n",
    "\n",
    "p_norm = [class_weights[np.argmax(i)] for i in y_test]\n",
    "\n",
    "sum = np.sum(p_norm)\n",
    "\n",
    "for i in range(len(p_norm)):\n",
    "  p_norm[i] /= sum\n",
    "\n",
    "class_weights_anom = {i: 0 for i in anom_classes}\n",
    "\n",
    "for i in y_data_anom:\n",
    "  class_weights_anom[i]+=1\n",
    "\n",
    "print(class_weights_anom)\n",
    "for i in list(class_weights_anom.keys()):\n",
    "  class_weights_anom[i] = len(y_data_anom) / class_weights_anom[i]\n",
    "\n",
    "p_anom = [class_weights_anom[i] for i in y_data_anom]\n",
    "\n",
    "\n",
    "sum = np.sum(p_anom)\n",
    "\n",
    "for i in range(len(p_anom)):\n",
    "  p_anom[i] /= sum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_anomaly_real_time(curves, host_galaxy, inds, model): # get real time scores for the indices inds. Inputs are curves and host_galaxy\n",
    "    # print('d')\n",
    "    splits = []\n",
    "    lcs = []\n",
    "    host_gals = []\n",
    "    for ind in inds:\n",
    "        cur = np.zeros((ntimesteps, 4))\n",
    "        anomaly_scores = []\n",
    "        host_gal = np.array(host_galaxy[ind])\n",
    "        curve = curves[ind]\n",
    "        \n",
    "        for ind, i in enumerate(curve):\n",
    "            if (np.count_nonzero(i) == 0):\n",
    "                break\n",
    "            cur[ind]=i\n",
    "\n",
    "            lcs.append(cur.copy())\n",
    "            host_gals.append(host_gal)\n",
    "    \n",
    "        splits.append(len(lcs))\n",
    "\n",
    "    lcs = np.array(lcs)\n",
    "    host_gals = np.array(host_gals)\n",
    "\n",
    "    scores = model.score(model.latent_model.predict([np.array(lcs), np.array(host_gals)]))\n",
    "    \n",
    "    ans = []\n",
    "    prv=0\n",
    "    for diff in splits:\n",
    "        ans.append(scores[prv:diff])\n",
    "        prv=diff\n",
    "    return ans\n",
    "\n",
    "def plot_real_median(X_test, x_data_anom, model, savepath=\"\", classes=file_to_class.values()):\n",
    "    \n",
    "    # assert(len(norm_scores) == len(anom_scores) and len(norm_scores) == 200)\n",
    "    \n",
    "    norm_median = []\n",
    "    norm_deviation = []\n",
    "    \n",
    "    class_median = {file_to_class[i]: [] for i in non_anom_classes}\n",
    "    class_deviation = {file_to_class[i]: [] for i in non_anom_classes}\n",
    "\n",
    "\n",
    "    cutoffs = list(range(-30, 71, 1))\n",
    "\n",
    "    for t in cutoffs:\n",
    "      cur_class = {file_to_class[i] : [] for i in non_anom_classes} # list of scores for each class at this cutoff\n",
    "        \n",
    "      cur_values = []\n",
    "      for ind in range(len(normal_inds)):\n",
    "\n",
    "        cutoff = cut_curve(np.copy(X_test[normal_inds[ind]]), (t+30)/100, ind=True)\n",
    "\n",
    "        # if (X_test[normal_inds[ind]][cutoff][1] <= (t+30-5)/100):\n",
    "        #   continue\n",
    "        cur_values.append(norm_scores[ind][min(cutoff, len(norm_scores[ind])-1)])\n",
    "        \n",
    "        cur_class[ordered_class_names[np.argmax(y_test[normal_inds[ind]])]].append(norm_scores[ind][min(cutoff, len(norm_scores[ind])-1)])\n",
    "        \n",
    "      for c in non_anom_classes:\n",
    "        # if (t == 0):\n",
    "            # print(np.mean(cur_class[c]))\n",
    "        # print(file_to_class[c], t, len(cur_class[c]))\n",
    "        cur_class[file_to_class[c]] = np.array(cur_class[file_to_class[c]])\n",
    "        class_median[file_to_class[c]].append(np.median(cur_class[file_to_class[c]]))\n",
    "        class_deviation[file_to_class[c]].append(np.median(np.absolute(cur_class[file_to_class[c]] - np.median(cur_class[file_to_class[c]]))))\n",
    "\n",
    "      cur_values=np.array(cur_values)\n",
    "\n",
    "      norm_median.append(np.median(cur_values))\n",
    "      norm_deviation.append(np.median(np.absolute(cur_values - np.median(cur_values))))\n",
    "\n",
    "    norm_median = np.array(norm_median)\n",
    "    norm_deviation = np.array(norm_deviation)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    for class_ in non_anom_classes:\n",
    "        class_ = file_to_class[class_]\n",
    "        class_median[class_] = np.array(class_median[class_])\n",
    "        class_deviation[class_] = np.array(class_deviation[class_])\n",
    "\n",
    "        if (class_ in classes):\n",
    "            ax.plot(cutoffs, class_median[class_], '-', label=class_, color=color_from_class[class_])\n",
    "            ax.fill_between(cutoffs, class_median[class_] - class_deviation[class_], class_median[class_] + class_deviation[class_], alpha=0.2, color=color_from_class[class_])\n",
    "\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    # return\n",
    "    \n",
    "\n",
    "    # Remove if you don't want majority aggregate\n",
    "    maj_col = u'grey'\n",
    "    ax.plot(cutoffs, norm_median, '-', label=\"Common Classes\", color=maj_col)\n",
    "    ax.fill_between(cutoffs, norm_median - norm_deviation, norm_median + norm_deviation, alpha=0.2, color=maj_col)\n",
    "    # ax.plot(cutoffs, norm_median, 'o', color=maj_col)\n",
    "\n",
    "\n",
    "    anom_median = []\n",
    "    anom_deviation = []\n",
    "\n",
    "    class_median = {i: [] for i in anom_classes}\n",
    "    class_deviation = {i: [] for i in anom_classes}\n",
    "\n",
    "    class_flux = {i : [] for i in anom_classes}\n",
    "    class_flux_dev = {i : [] for i in anom_classes}\n",
    "\n",
    "    for t in cutoffs:\n",
    "\n",
    "      cur_values = []\n",
    "      cur_class = {i : [] for i in anom_classes}\n",
    "      cur_flux = {i : [] for i in anom_classes}\n",
    "      for ind in range(len(anom_inds)):\n",
    "        cutoff = cut_curve(np.copy(x_data_anom[anom_inds[ind]]), (t+30)/100, ind=True)\n",
    "\n",
    "        \n",
    "        cur_values.append(anom_scores[ind][min(cutoff, len(anom_scores[ind])-1)])\n",
    "        \n",
    "        cur_class[y_data_anom[anom_inds[ind]]].append(anom_scores[ind][min(cutoff, len(anom_scores[ind])-1)])\n",
    "          \n",
    "        cur_flux[y_data_anom[anom_inds[ind]]].append(x_data_anom[ind][min(cutoff, len(anom_scores[ind])-1)])\n",
    "        \n",
    "      for c in anom_classes:\n",
    "\n",
    "        cur_class[c] = np.array(cur_class[c])\n",
    "        class_median[c].append(np.median(cur_class[c]))\n",
    "        class_deviation[c].append(np.median(np.absolute(cur_class[c] - np.median(cur_class[c]))))\n",
    "\n",
    "        cur_flux[c] = np.array(cur_flux[c])\n",
    "        class_flux[c].append(np.median(cur_flux[c]))\n",
    "        class_flux_dev[c].append(np.median(np.absolute(cur_flux[c] - np.median(cur_flux[c]))))\n",
    "\n",
    "      cur_values=np.array(cur_values)\n",
    "\n",
    "      anom_median.append(np.median(cur_values))\n",
    "      anom_deviation.append(np.median(np.absolute(cur_values - np.median(cur_values))))\n",
    "\n",
    "    anom_median = np.array(anom_median)\n",
    "    anom_deviation = np.array(anom_deviation)\n",
    "\n",
    "    # Remove this if you don't want anomalous aggregate\n",
    "    ax.plot(cutoffs, anom_median, '-', label=\"Anomalous\", color=u'#ff7f0e')\n",
    "    ax.fill_between(cutoffs, anom_median - anom_deviation, anom_median + anom_deviation, alpha=0.2, color=u'#ff7f0e')\n",
    "\n",
    "    \n",
    "    for ind, c in enumerate(anom_classes):\n",
    "        if (file_to_class[c] not in classes and c not in classes):\n",
    "            continue\n",
    "\n",
    "    \n",
    "        ax.plot(cutoffs, class_median[c], '-', label=file_to_class[c],  color=color_from_class[c])\n",
    "        ax.fill_between(cutoffs, np.array(class_median[c]) - np.array(class_deviation[c]), np.array(class_median[c]) + np.array(class_deviation[c]), alpha=0.2,  color=color_from_class[c])\n",
    "\n",
    "    ax.set_xlabel(\"Time\", fontsize=25)\n",
    "    ax.set_ylabel(\"Median Anomaly Score\", fontsize=25)\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    \n",
    "\n",
    "    # ax.set_title(\"Median Anomaly Score Over Time\", fontsize=28)\n",
    "    # plt.show()\n",
    "    saveplot(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the random sample of inds/scores\n",
    "\n",
    "normal_inds = np.random.choice(list(range(len(X_test))), 2000, p=p_norm, replace=False)\n",
    "anom_inds = np.random.choice(list(range(len(x_data_anom))), 2000, p = p_anom, replace=False)\n",
    "\n",
    "anom_scores = get_anomaly_real_time(x_data_anom, host_gal_anom, anom_inds, best)\n",
    "\n",
    "norm_scores = get_anomaly_real_time(X_test, host_gal_test, normal_inds, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "# plot_real_median(X_test, x_data_anom, best, \"last/MajScale\", ['SNIa', 'SNIIn', 'SLSN-I', 'KNe'])\n",
    "plot_real_median(X_test, x_data_anom, best, \"last/AnomScale\", anom_classes) # ['SNIa', 'SNIIn', 'SLSN-I', 'KNe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_median(X_test, x_data_anom, best, \"last/AllScale\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following 2 codeblocks are really just for debugging. Use them for sample usage of the plot_real_time function (which is defined 3 blocks below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "\n",
    "ind = 100\n",
    "import random\n",
    "x_data_div = x_data_anom.copy()\n",
    "\n",
    "    \n",
    "cnt = 0\n",
    "while True:\n",
    "\n",
    "    if (file_to_class[y_data_anom[ind]] == 'uLens-BSR'):\n",
    "        plot_real_time(get_anomaly_real_time(x_data_div, host_gal_anom, [ind], best)[0], x_data_div[ind], file_to_class[y_data_anom[ind]])\n",
    "\n",
    "        if (cnt == 10):\n",
    "            break\n",
    "        \n",
    "        \n",
    "    ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "import random\n",
    "ind = 0\n",
    "\n",
    "cnt = 0\n",
    "while True:\n",
    "\n",
    "    if (ordered_class_names[np.argmax(y_test[ind])] == 'SNIa'):\n",
    "        cnt += 1\n",
    "        if (cnt == 10):\n",
    "            break\n",
    "\n",
    "        plot_real_time(get_anomaly_real_time(X_test, np.zeros((len(X_test), 2)), [ind], best)[0], X_test[ind], ordered_class_names[np.argmax(y_test[ind])])\n",
    "\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_time(classification_scores, curve, class_):\n",
    "    cur = np.array([(j[1] * 100) - 30 for j in curve])\n",
    "    assert(not np.any(curve[len(classification_scores)]))\n",
    "    cur = cur[:len(classification_scores)]\n",
    " \n",
    "    r_band = []\n",
    "    rer = []\n",
    "    r_cur = []\n",
    "    g_band = []\n",
    "    ger = []\n",
    "    g_cur = []\n",
    "\n",
    "    for i in curve[:len(classification_scores)]:\n",
    "        if (i[0] < 0.5):\n",
    "            r_band.append(i[2])\n",
    "            rer.append(i[3])\n",
    "            r_cur.append((i[1] * 100) - 30)\n",
    "\n",
    "        else:\n",
    "            g_band.append(i[2])\n",
    "            ger.append(i[3])\n",
    "            g_cur.append((i[1] * 100) - 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2, figsize=(10, 20))\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    axs[0].set_title(f\"Real Time Anomaly Score for a {class_}\", fontsize=30)\n",
    "\n",
    "\n",
    "    axs[0].set_ylabel('Scaled Flux', fontsize=27)\n",
    "    axs[0].errorbar(r_cur, r_band, yerr=rer, fmt='.', label= 'r band')\n",
    "    axs[0].errorbar(g_cur, g_band, yerr = ger, fmt = '.', label= 'g band')\n",
    "\n",
    "\n",
    "    axs[1].set_ylabel('Anomaly Score', fontsize=27)\n",
    "    axs[1].set_xlabel('Time Since Trigger', fontsize=27)\n",
    "    axs[1].plot(cur, classification_scores)\n",
    "\n",
    "    axs[1].set_ylim(-0.3, 0.3)\n",
    "    axs[1].set_yticks(ticks=np.arange(-0.3, 0.3, 0.1))\n",
    "\n",
    "    axs[0].tick_params(axis='both', labelsize=27)\n",
    "    axs[1].tick_params(axis='both', labelsize=27)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # axs[1].axhline(0.05, linestyle='--', color='r')\n",
    "\n",
    "    axs[1].legend()\n",
    "    plt.axvline(0, linestyle='--', color='r')\n",
    "\n",
    "    saveplot(f\"NewEx/{class_}Example{random.randint(1, 1000)}\")\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next part is only there to produce a pretty plot for the paper :). I would ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = {\n",
    "    \"SNIa\": 0,\n",
    "    \"SNIa-91bg\": 11598,\n",
    "    \"SNIax\":24800,\n",
    "    \"SNIb\":43171,\n",
    "    \"SNIc\":48450,\n",
    "    \"SNIc-BL\":50052,\n",
    "    \"SNII\":51471,\n",
    "    \"SNIIn\":78884,\n",
    "    \"SNIIb\":97091,\n",
    "    \"TDE\":109392,\n",
    "    \"SLSN-I\":120754,\n",
    "    \"AGN\":138142,\n",
    "    \"CaRT\":144631,\n",
    "    \"KNe\":154531,\n",
    "    \"PISN\":166504,\n",
    "    \"ILOT\":177026,\n",
    "    \"uLens-BSR\":187880\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding good examples\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(20, 30), sharey=True)\n",
    "# fig.subplots_adjust(hspace=0)\n",
    "\n",
    "\n",
    "done = {i : False for i in file_names}\n",
    "\n",
    "def plot_curves(name, num = 100):\n",
    "\n",
    "    band_medians = {'r' : 0.4827, 'g' : 0.6223}\n",
    "    cnt = 0\n",
    "\n",
    "    for ind in range(len(x_data)):\n",
    "\n",
    "        if (target[ind] == name and cut_curve(x_data[ind], 1, ind=True) > 55 and host_galaxy_info[ind][0] < 0.5):\n",
    "            \n",
    "            maxi = np.max(x_data[ind][:, 2])\n",
    "            mini = np.min(x_data[ind][:, 2])\n",
    "            x_data[ind][:, 2] = (x_data[ind][:, 2] - mini) / (maxi - mini)\n",
    "            x_data[ind][:, 3] /= (maxi - mini)\n",
    "\n",
    "            done[target[ind]] = True\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "            red = x_data[ind][x_data[ind][:, 0] == band_medians['r']]\n",
    "            green = x_data[ind][x_data[ind][:, 0] == band_medians['g']]\n",
    "            plt.errorbar(red[:, 1], ((red[:, 2] + 30) * 100), fmt='^', alpha=0.3, color = colors[file_names.index(target[ind]) % len(colors)])\n",
    "            plt.errorbar(green[:, 1], ((green[:, 2] + 30) * 100), fmt='o', alpha=1, color = colors[file_names.index(target[ind])  % len(colors)])\n",
    "\n",
    "            # axs[int(cnt > 8)].errorbar(new_vals[:, 1] * 100 - 30, new_vals[:, 2] + prv_mx, fmt='o', color = colors[file_names.index(target[ind]) % len(colors)])\n",
    "            # plt.text(0, prv_mx + x_data[0][2] + 0.5, file_to_class[target[ind]], fontsize=20, color = colors[file_names.index(target[ind]) % len(colors)])\n",
    "\n",
    "            # prv_mx += np.max(new_vals[:, 2]) + 0.5\n",
    "\n",
    "            \n",
    "            plt.ylabel('Scaled Flux + Offset', fontsize=15)\n",
    "            plt.xlabel('Time Since Trigger', fontsize=15)\n",
    "            plt.title(str(ind))\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "        if (cnt == 100):\n",
    "            break\n",
    "    \n",
    "    # plt.title('Sample Light Curves')\n",
    "    # plt.ylim(-1, 29)\n",
    "    # saveplot(\"Figures/samplecurves\")\n",
    "    \n",
    "plot_curves(class_to_file['uLens-BSR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_inds = list(inds.values()) # [6, 13024, 26037, 39092, 44415, 45914, 47312, 60322, 73318, 85658, 97110, 114369, 120846, 130758, 144661, 153253, 164107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_file = {v: k for k, v in file_to_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, value in file_to_class.items():\n",
    "    color_from_class[value] = color_from_class[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(x_data)):\n",
    "    if (file_to_class[target[i]] == 'SNIa'):\n",
    "        plot_curve(x_data[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 40))\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(20, 30), sharey=True)\n",
    "# fig.subplots_adjust(hspace=0)\n",
    "\n",
    "\n",
    "done = {i : False for i in file_names}\n",
    "\n",
    "\n",
    "\n",
    "band_medians = {'r' : 0.4827, 'g' : 0.6223}\n",
    "prv_mx = 0\n",
    "cnt = 0\n",
    "\n",
    "for ind in reversed(interesting_inds): # [6, 13024, 26037, 39092, 44415, 45914, 47312, 60322, 73318, 97110, 85658, 114369, 130758, 144661, 153253, 164107, 120846]:\n",
    "    # print(ind)\n",
    "    maxi = np.max(x_data[ind][:, 2])\n",
    "    mini = np.min(x_data[ind][:, 2])\n",
    "    x_data[ind][:, 2] = (x_data[ind][:, 2] - mini) / (maxi - mini)\n",
    "    x_data[ind][:, 3] /= (maxi - mini)\n",
    "\n",
    "    done[target[ind]] = True\n",
    "    cnt += 1\n",
    "    if (cnt >= 10):\n",
    "        break\n",
    "\n",
    "    red = x_data[ind][x_data[ind][:, 0] == band_medians['r']]\n",
    "    green = x_data[ind][x_data[ind][:, 0] == band_medians['g']]\n",
    "    plt.errorbar((red[:, 1]) * 100 - 30, red[:, 2] + prv_mx, fmt='^', alpha=0.3, color = color_from_class[file_to_class[target[ind]]])\n",
    "    plt.errorbar((green[:, 1]) * 100 - 30, green[:, 2] + prv_mx, fmt='o', alpha=1, color = color_from_class[file_to_class[target[ind]]])\n",
    "\n",
    "    # axs[int(cnt > 8)].errorbar(new_vals[:, 1] * 100 - 30, new_vals[:, 2] + prv_mx, fmt='o', color = colors[file_names.index(target[ind]) % len(colors)])\n",
    "    plt.text(5, prv_mx + 0.5, file_to_class[target[ind]] + \"        \", fontsize=25, color = color_from_class[file_to_class[target[ind]]], ha='right', va='bottom')\n",
    "\n",
    "    prv_mx += 1.2\n",
    "\n",
    "        \n",
    "plt.ylabel('Scaled Flux + Offset', fontsize=25)\n",
    "plt.xlabel('Time Since Trigger', fontsize=25)\n",
    "\n",
    "# plt.title('Sample Light Curves')\n",
    "# plt.ylim(-1, 29)\n",
    "saveplot(\"last/samplecurves2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Testing with a Single Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# recalculate class weights\n",
    "\n",
    "class_weights = {i : 0 for i in range(y_train.shape[1])}\n",
    "\n",
    "for value in y_train:\n",
    "  class_weights[np.argmax(value)]+=1\n",
    "\n",
    "for value in y_val:\n",
    "  class_weights[np.argmax(value)]+=1\n",
    "\n",
    "for id in class_weights.keys():\n",
    "  class_weights[id] = (len(y_train) + len(y_val)) / class_weights[id]\n",
    "\n",
    "sample_weights = [class_weights[np.argmax(i)] for i in y_train] + [class_weights[np.argmax(i)] for i in y_val]\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "best.iforest = IsolationForest(random_state=0, max_samples = 'auto', n_estimators=200).fit(np.append(best.train_latent, best.val_latent, axis=0), sample_weight=sample_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best.single_scores = -best.iforest.decision_function(best.test_latent)\n",
    "best.single_scores_anom = -best.iforest.decision_function(best.anom_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average_score(best.single_scores, y_test, best.single_scores_anom, y_data_anom, title=\"Average Anomaly Score\\n(Single Isolation Forest)\", savepath=\"FiguresNew/AverageScoreSingle\")\n",
    "median_score(best.single_scores, y_test, best.single_scores_anom, y_data_anom, title=\"Median Anomaly Score\\n(Single Isolation Forest)\", savepath=\"FiguresNew/MedianScoreSingle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dist(best.single_scores, y_test, best.single_scores_anom, y_data_anom, title=\"Anomaly Score Distribution\\n(Single Isolation Forest)\", savepath=\"FiguresNew/ScoreDistSingle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_real(curves, host_galaxy, inds, model): # Real time classification scores. Input is similar to real time anomaly scores\n",
    "    # print('d')\n",
    "    splits = []\n",
    "    lcs = []\n",
    "    host_gals = []\n",
    "    for ind in inds:\n",
    "        cur = np.zeros((ntimesteps, 4))\n",
    "        anomaly_scores = []\n",
    "        host_gal = np.array(host_galaxy[ind])\n",
    "        curve = curves[ind]\n",
    "        \n",
    "        for ind, i in enumerate(curve):\n",
    "            if (np.count_nonzero(i) == 0):\n",
    "                break\n",
    "            cur[ind]=i\n",
    "\n",
    "            lcs.append(cur.copy())\n",
    "            host_gals.append(host_gal)\n",
    "    \n",
    "        splits.append(len(lcs))\n",
    "\n",
    "    lcs = np.array(lcs)\n",
    "    host_gals = np.array(host_gals)\n",
    "\n",
    "    scores = model.model.predict([np.array(lcs), np.array(host_gals)])\n",
    "    \n",
    "    ans = []\n",
    "    prv=0\n",
    "    for diff in splits:\n",
    "        ans.append(scores[prv:diff])\n",
    "        prv=diff\n",
    "    return ans\n",
    "\n",
    "def get_roc_auc(predictions, ind, labels, savepath=None):\n",
    "    \n",
    "    predictions=np.array(predictions)\n",
    "    fpr, tpr, _ = roc_curve(y_true = labels, y_score = predictions[:, ind], pos_label = ind)\n",
    "\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "def plot_real_auc(X_test, x_data_anom, model, savepath=\"\", classes=file_to_class.values()): # Plot real-time AUROCs, should be easy\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    class_auc = {file_to_class[i]: [] for i in non_anom_classes}\n",
    "\n",
    "\n",
    "    cutoffs = list(range(-30, 71, 1))\n",
    "    \n",
    "    labels = [np.argmax(y_test[i]) for i in normal_inds]\n",
    "    \n",
    "    for t in cutoffs:\n",
    "      here = []\n",
    "      for ind in range(len(normal_inds)):\n",
    "    \n",
    "        cutoff = cut_curve(np.copy(X_test[normal_inds[ind]]), (t+30)/100, ind=True)\n",
    "\n",
    "        here.append(norm_scores[ind][min(cutoff, len(norm_scores[ind])-1)])\n",
    "    \n",
    "      for ind, cl in enumerate(ordered_class_names):\n",
    "        class_auc[cl].append(get_roc_auc(here, ind, labels))\n",
    "\n",
    "\n",
    "    for c in range(len(ordered_class_names)):\n",
    "        if (ordered_class_names[c] in classes):\n",
    "            plt.plot(cutoffs, class_auc[ordered_class_names[c]], label=ordered_class_names[c], color = color_from_class[ordered_class_names[c]])\n",
    "\n",
    "    # return\n",
    "    plt.legend(fontsize=18)\n",
    "\n",
    "    plt.xticks(fontsize=22)\n",
    "    plt.yticks(fontsize=22)\n",
    "    \n",
    "    plt.ylabel(\"AUROC\", fontsize=27)\n",
    "    plt.xlabel(\"Time\", fontsize=27)\n",
    "    plt.title(\"Classification AUROC Over Time\", fontsize=29)\n",
    "    # plt.ylim(-0.1, 0.1)\n",
    "    \n",
    "\n",
    "    # ax.set_title(\"Median Anomaly Score Over Time\", fontsize=28)\n",
    "    # plt.show()\n",
    "    saveplot(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_inds = np.random.choice(list(range(len(X_test))), 2000, p=p_norm, replace=False)\n",
    "norm_scores = get_classification_real(X_test, host_gal_test, normal_inds, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_auc(X_test, x_data_anom, best, classes = [\"SLSN-I\", \"SNIIn\", \"SNIa\"], savepath = \"last/realclassmaj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, TimeDistributed, Dense, Masking, concatenate, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class Detect:\n",
    "    ntimesteps=656\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, x_data):\n",
    "        for ind in range(len(x_data)):\n",
    "            np.pad(x_data[ind], ((0, cls.ntimesteps - len(x_data[ind])), (0, 0)))\n",
    "        return x_data\n",
    "\n",
    "    @classmethod\n",
    "    def init(cls):\n",
    "        with open(\"pretrained\", 'rb') as f:\n",
    "            cls.mod = pickle.load(f)\n",
    "\n",
    "        \n",
    "        # cls.mod = Custom(ntimesteps, 4, 2, 9, y_train.shape[-1])\n",
    "        # cls.mod.custom_model(best.model, 'lc', 'host', 'latent')\n",
    "        # cls.mod.create_encoder()\n",
    "        # cls.mod.mcif = mcif()\n",
    "        # cls.mod.mcif.iforests = best.iso_forests\n",
    "\n",
    "    @classmethod\n",
    "    def classify(cls, x_data, host_gal):\n",
    "        return cls.mod.classify(x_data, host_gal)\n",
    "\n",
    "    @classmethod\n",
    "    def anomaly_score(cls, x_data, host_gal):\n",
    "        return cls.mod.score(x_data, host_gal)\n",
    "\n",
    "    @classmethod\n",
    "    def plot_real_time(cls, x_data, host_gal):\n",
    "        cls.mod.plot_real_time(x_data, host_gal, [0.4827, 0.6223], x_data[:, 1] * 100 - 30, x_data[:, 2] * 500, x_data[:, 3] * 500, colors=['red', 'g'], names=['r', 'g'])\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "class Custom:\n",
    "    def __init__(self, timesteps, features, contextual, latent_size, n_classes):\n",
    "        self.n_classes=n_classes\n",
    "        self.features=features\n",
    "        self.contextual=contextual\n",
    "        self.latent_size = latent_size\n",
    "        self.timesteps=timesteps\n",
    "\n",
    "    def pad(self, x_data):\n",
    "        for ind in range(len(x_data)):\n",
    "            np.pad(x_data[ind], ((0, self.timesteps - len(x_data[ind])), (0, 0)))\n",
    "        return x_data\n",
    "\n",
    "    def create_model(self):\n",
    "        input_1 = Input((self.timesteps, self.features), name='lc')  # X.shape = (Nobjects, Ntimesteps, 4) CHANGE\n",
    "        self.lc_name = 'lc'\n",
    "        masking_input1 = Masking(mask_value=0.)(input_1)\n",
    "    \n",
    "        lstm1 = GRU(100, return_sequences=True, activation='tanh')(masking_input1)\n",
    "        lstm2 = GRU(100, return_sequences=False, activation='tanh')(lstm1)\n",
    "    \n",
    "        dense1 = Dense(100, activation='tanh')(lstm2)\n",
    "\n",
    "        if (self.contextual > 0):\n",
    "            input_2 = Input(shape = (self.contextual, ), name='host') # CHANGE\n",
    "            self.context_name = 'host'\n",
    "            dense2 = Dense(10)(input_2)\n",
    "            merge1 = concatenate([dense1, dense2])\n",
    "\n",
    "        else:\n",
    "            merge1 = dense1\n",
    "    \n",
    "        dense3 = Dense(100, activation='relu')(merge1)\n",
    "    \n",
    "        dense4 = Dense(self.latent_size, activation='relu', name='latent')(dense3)\n",
    "    \n",
    "        output = Dense(self.n_classes, activation='softmax')(dense4)\n",
    "\n",
    "        if (self.contextual):\n",
    "            self.model = keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "        else:\n",
    "            self.model = keras.Model(inputs=[input_1], outputs=output)\n",
    "    \n",
    "    \n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "        \n",
    "        self.latent_name='latent'\n",
    "        \n",
    "    def custom_model(self, model, lc_name, context_name, latent_name):\n",
    "        self.model=model\n",
    "        self.lc_name = lc_name\n",
    "        self.context_name = context_name\n",
    "        self.latent_name=latent_name\n",
    "        \n",
    "    def train(self, X_train, y_train, x_val, y_val, savepath, host_gal_train = None, host_gal_val = None):\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "                                      patience=5,\n",
    "                                      min_delta=0.001,                               \n",
    "                                      monitor=\"val_loss\",\n",
    "                                      restore_best_weights=True\n",
    "                                      )\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (self.contextual > 0):\n",
    "            self.history = self.model.fit(x = [X_train, host_gal_train], validation_data=([X_val, host_gal_val], y_val), y = y_train, epochs=40, batch_size = 128, class_weight = class_weights, callbacks=[early_stopping])\n",
    "        else:\n",
    "            self.history = self.model.fit(x = [X_train], validation_data=([X_val], y_val), y = y_train, epochs=40, batch_size = 128, class_weight = class_weights, callbacks=[early_stopping])\n",
    "            \n",
    "        \n",
    "    \n",
    "    def create_encoder(self):\n",
    "        if (self.contextual):\n",
    "            self.latent_model = Model(inputs=[self.model.get_layer(self.lc_name).input, self.model.get_layer(self.context_name).input], outputs=self.model.get_layer(self.latent_name).output)\n",
    "        else:\n",
    "            self.latent_model = Model(inputs=[self.model.get_layer(self.lc_name).input], outputs=self.model.get_layer(self.latent_name).output)\n",
    "            \n",
    "\n",
    "    def predict(self, x_data, host_gal=None):\n",
    "        if (self.contextual > 0):\n",
    "            return self.model.predict(x = [x_data, host_gal])\n",
    "        else:\n",
    "            return self.model.predict(x = [x_data])\n",
    "        \n",
    "        \n",
    "    def encode(self, x_data, host_gal=None):\n",
    "        if (self.contextual > 0):\n",
    "            return self.latent_model.predict(x = [x_data, host_gal])\n",
    "        else:\n",
    "            return self.latent_model.predict(x = [x_data])\n",
    "    \n",
    "    def init_mcif(self, x_data, y_data, host_gal, n_estimators=100):\n",
    "        self.mcif = mcif(n_estimators)\n",
    "        self.mcif.train(self.encode(x_data, host_gal), y_data)\n",
    "        \n",
    "        \n",
    "\n",
    "    def score(self, x_data, host_gal=None):\n",
    "        return self.mcif.score(self.encode(x_data, host_gal))\n",
    "\n",
    "    def get_anomaly_real_time(self, curves, host_galaxy=None):\n",
    "        \n",
    "        splits = []\n",
    "        lcs = []\n",
    "        host_gals = []\n",
    "        for ind in range(len(curves)):\n",
    "            cur = np.zeros((self.timesteps, 4))\n",
    "            anomaly_scores = []\n",
    "            if (self.contextual):\n",
    "                host_gal = np.array(host_galaxy[ind])\n",
    "            \n",
    "            curve = curves[ind]\n",
    "            \n",
    "            for ind, i in enumerate(curve):\n",
    "                if (np.count_nonzero(i) == 0):\n",
    "                    break\n",
    "                cur[ind]=i\n",
    "    \n",
    "                lcs.append(cur.copy())\n",
    "                if (self.contextual):\n",
    "                    host_gals.append(host_gal)\n",
    "        \n",
    "            splits.append(len(lcs))\n",
    "    \n",
    "        lcs = np.array(lcs)\n",
    "        host_gals = np.array(host_gals)\n",
    "    \n",
    "        scores = self.score(np.array(lcs), np.array(host_gals))\n",
    "        \n",
    "        ans = []\n",
    "        prv=0\n",
    "        for diff in splits:\n",
    "            ans.append(scores[prv:diff])\n",
    "            prv=diff\n",
    "        \n",
    "        return ans\n",
    "\n",
    "\n",
    "    def plot_real_time(self, x_data, host_gal, bands, time, flux, error, names = [], colors = []):\n",
    "        cur = np.array([j[1] for j in x_data])\n",
    "        classification_scores = self.get_anomaly_real_time([x_data], [host_gal])[0]\n",
    "        \n",
    "        cur = cur[:len(classification_scores)]\n",
    "        assert(len(cur) == len(classification_scores))\n",
    "\n",
    "        time = {i : [] for i in bands}\n",
    "        flux = {i : [] for i in bands}\n",
    "        error = {i : [] for i in bands}\n",
    "\n",
    "        \n",
    "    \n",
    "        for i in x_data[:len(classification_scores)]:\n",
    "            if (not np.any(i)):\n",
    "                break\n",
    "            flux[i[0]].append(i[2])\n",
    "            error[i[0]].append(i[3])\n",
    "            time[i[0]].append(i[1])\n",
    "    \n",
    "        fig, axs = plt.subplots(2, figsize=(10, 20))\n",
    "    \n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "        axs[0].set_title(f\"Real Time Anomaly Score\", fontsize=30)\n",
    "    \n",
    "    \n",
    "        axs[0].set_ylabel('Flux', fontsize=27)\n",
    "        for ind, i in enumerate(bands):\n",
    "            axs[0].errorbar(time[i], flux[i], yerr=error[i], fmt='.', label = names[ind] if len(names) else None, color = colors[ind] if len(colors) else None)\n",
    "    \n",
    "    \n",
    "        axs[1].set_ylabel('Anomaly Score', fontsize=27)\n",
    "        axs[1].set_xlabel('Time Since Trigger', fontsize=27)\n",
    "        axs[1].plot(cur, classification_scores)\n",
    "    \n",
    "        axs[1].set_ylim(-0.3, 0.3)\n",
    "        axs[1].set_yticks(ticks=np.arange(-0.3, 0.3, 0.1))\n",
    "    \n",
    "        axs[0].tick_params(axis='both', labelsize=27)\n",
    "        axs[1].tick_params(axis='both', labelsize=27)\n",
    "        \n",
    "        axs[0].legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "class mcif:\n",
    "    def __init__(self, n_estimators = 100):\n",
    "        self.n_estimators=n_estimators\n",
    "\n",
    "    def train(self, x_data, labels):\n",
    "        self.classes = np.unique(labels, axis=0)\n",
    "        self.iforests = [IsolationForest(n_estimators=self.n_estimators) for i in self.classes]\n",
    "        \n",
    "        for ind, cls in enumerate(self.classes):\n",
    "            here = []\n",
    "            for i in range(len(x_data)):\n",
    "                if (list(cls) == list(labels[i])):\n",
    "                    here.append(x_data[i])\n",
    "\n",
    "            self.iforests[ind].fit(here)\n",
    "            \n",
    "\n",
    "    def score_discrete(self, data):\n",
    "        scores = [-det.decision_function(data) for det in self.iforests]\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        scores = scores.T\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def score(self, data):\n",
    "        return [np.min(i) for i in self.score_discrete(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astromcad.astromcad import Custom, mcif, Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install astromcad\n",
    "\n",
    "test = Custom(656, 4, 0, 9, 12)\n",
    "test.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.mcif = mcif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.mcif.`(best.train_latent, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.create_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save(\"pretrained\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.plot_real_time(X_train[0], host_gal[0], [0.4827, 0.6223], X_train[0][:, 1], X_train[0][:, 2], X_train[0][:, 3], ['r', 'g'], ['red', 'green'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test.score(np.array([X_train[0]]), np.array([host_gal[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astromcad.astromcad import Detect\n",
    "Detect.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detect.plot_real_time(X_train[0], host_gal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test.score_discreet(best.train_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
