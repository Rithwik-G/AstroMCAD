{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqjYxfAyihYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13458642-fc23-4d6f-ff5c-12899004b42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap\n",
            "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: umap\n",
            "  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3541 sha256=ee2df55ca92dd50ab44239f0b171696b31f62ee9b4653b946965b0eb149fda9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/f1/28/53dcf7a309118ed35d810a5f9cb995217800f3f269ab5771cb\n",
            "Successfully built umap\n",
            "Installing collected packages: umap\n",
            "Successfully installed umap-0.1.1\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (0.29.36)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->hdbscan) (3.2.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039165 sha256=ccd791814331a6f76d46dd6d8bc21e03472463a7f420553b9a30babb982e59a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.33\n"
          ]
        }
      ],
      "source": [
        "!pip3 install umap\n",
        "!pip3 install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import umap\n",
        "import hdbscan\n",
        "import keras\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOGRUcAwjInY",
        "outputId": "c126c6d3-e4a5-4195-bd6f-5f8faf74fedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = []\n",
        "file_names = []\n",
        "\n",
        "for filename in os.listdir(\"lightcurves\"):\n",
        "  file_names.append(filename)\n",
        "\n",
        "  data = pickle.load(open(f\"lightcurves/{filename}\", \"rb\"))\n",
        "  full_data.append(data)"
      ],
      "metadata": {
        "id": "kVWuwpVfjYJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [] # shape => for each time step, store time, median passband wavelength, flux, flux error\n",
        "host_galaxy_info = []\n",
        "target = [] # store target class"
      ],
      "metadata": {
        "id": "PtB6jwhGjkV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "band_medians = {'r' : 0.4827, 'g' : 0.6223} # Median Wavelength (in Angstroms) Scaled over 10000 for ZTF\n",
        "\n",
        "\n",
        "before = 30\n",
        "after = 70\n",
        "\n",
        "for ind, filename in enumerate(file_names):\n",
        "\n",
        "  data = full_data[ind]\n",
        "\n",
        "  ids = list(data.keys())\n",
        "\n",
        "  for id in ids:\n",
        "\n",
        "    cur_meta = [data[id].meta['redshift'], data[id].meta['mwebv']] # host gal info, redshift and extinction\n",
        "    df = (pd.DataFrame(np.array(data[id])))\n",
        "\n",
        "    for i in range(5): # Sigma Clipping\n",
        "      mean = np.mean(df['fluxErr'])\n",
        "      sigma = np.std(df['fluxErr'])\n",
        "\n",
        "      min = mean - 3*sigma\n",
        "      max = mean + 3*sigma\n",
        "\n",
        "      df = df[((df['fluxErr'] <= max) & (df['fluxErr'] >= min))]\n",
        "\n",
        "    cur_meta.extend([np.max(df[df['passband'] == 'r']['flux']), np.max(df[df['passband'] == 'g']['flux'])]) # Peak flux in each passband, unscaled\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    denom = np.max(df['flux']) - np.min(df['flux'])\n",
        "    df.fluxErr = df['fluxErr'] / denom # Scale Flux\n",
        "\n",
        "    df.flux = scaler.fit_transform(np.array(df['flux']).reshape(-1, 1)).flatten()\n",
        "\n",
        "    trigger_mjd = 0\n",
        "\n",
        "    df = df[(df['time'] > trigger_mjd - before) & (df['time'] < trigger_mjd + after)] # Scale Time\n",
        "\n",
        "\n",
        "    if (len(df[df['time'] < 0]) < 2):\n",
        "      continue\n",
        "\n",
        "    df.sort_values(\"time\", inplace=True)\n",
        "    df.drop('photflag', axis=1, inplace=True)\n",
        "\n",
        "    df.time = (df.time - (-before)) / (after + before)\n",
        "\n",
        "\n",
        "    df['passband'] = df['passband'].map(band_medians)\n",
        "\n",
        "    x_data.append(np.array(df))\n",
        "    target.append(filename)\n",
        "    host_galaxy_info.append(cur_meta)"
      ],
      "metadata": {
        "id": "9EufAJ19jTbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "target = load(\"/content/drive/My Drive/plasticc_train_data/preprocesseddata/target\")\n",
        "x = load(\"/content/drive/My Drive/plasticc_train_data/preprocesseddata/x\")\n",
        "host_galaxy_info = load(\"/content/drive/My Drive/plasticc_train_data/preprocesseddata/host_galaxy_info\")\n"
      ],
      "metadata": {
        "id": "gPD_0u79jQrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = np.unique(target)\n",
        "\n",
        "anom_inds = [1, 5, 12, 13, 16]\n",
        "for i in anom_inds:\n",
        "    print(classes[i])\n",
        "\n",
        "# Indices of anomalous classes"
      ],
      "metadata": {
        "id": "yGtMAGMUlIYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cut everything with less than 10 points and get length of all lc's\n",
        "\n",
        "lengths = []\n",
        "delete = []\n",
        "\n",
        "for ind, val in enumerate(x):\n",
        "  if (len(val) < 10):\n",
        "    delete.append(ind)\n",
        "  lengths.append(len(val))"
      ],
      "metadata": {
        "id": "U8mcB9m4lPOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(delete) - 1, -1, -1):\n",
        "    del x[delete[i]]\n",
        "    del target[delete[i]]\n",
        "    del host_galaxy_info[delete[i]]"
      ],
      "metadata": {
        "id": "33u6gG95lZvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log of Peak Flux\n",
        "\n",
        "host_galaxy_info = np.array(host_galaxy_info)\n",
        "\n",
        "host_galaxy_info[:, 3] = np.log(host_galaxy_info[:, 3])\n",
        "host_galaxy_info[:, 2] = np.log(host_galaxy_info[:, 2])"
      ],
      "metadata": {
        "id": "0q9_8hD3laRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shouldn't be negative\n",
        "\n",
        "delete = []\n",
        "\n",
        "for ind, i in enumerate(host_galaxy_info):\n",
        "    if (np.isnan(host_galaxy_info[ind][2]) or np.isnan(host_galaxy_info[ind][3])):\n",
        "        delete.append(ind)"
      ],
      "metadata": {
        "id": "iBa9h8vSlpDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "host_galaxy_info = list(host_galaxy_info)\n",
        "\n",
        "for i in range(len(delete) - 1, -1, -1):\n",
        "\n",
        "    del x[delete[i]]\n",
        "    del target[delete[i]]\n",
        "    del host_galaxy_info[delete[i]]"
      ],
      "metadata": {
        "id": "iG5IEJemlu6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "host_galaxy_info = np.array(host_galaxy_info)\n",
        "host_galaxy_info[:, 2] = host_galaxy_info[:, 2] / 10\n",
        "host_galaxy_info[:, 3] = host_galaxy_info[:, 3] / 10\n",
        "\n",
        "# Divide peaks by 10 for further scaling"
      ],
      "metadata": {
        "id": "u8jGBGm4l4Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad for TF masking layer\n",
        "\n",
        "ntimesteps = np.max(lengths)\n",
        "\n",
        "for ind in range(len(x)):\n",
        "  x[ind] = np.pad(x[ind], ((0, ntimesteps - len(x[ind])), (0, 0)))"
      ],
      "metadata": {
        "id": "iSv-PFrol7am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "\n",
        "y_data_anom = []\n",
        "y_data = []\n",
        "x_data = []\n",
        "x_data_anom = []\n",
        "host_gal_anom = []\n",
        "host_gal = []\n",
        "\n",
        "anom_classes = [classes[i] for i in anom_inds]\n",
        "\n",
        "for i in range(len(target)):\n",
        "    if (target[i] == 'lc_classnum_AGN_old.pickle'): # ignore AGNS\n",
        "        continue\n",
        "    if (target[i] in anom_classes):\n",
        "        x_data_anom.append(x[i])\n",
        "        y_data_anom.append(target[i])\n",
        "        host_gal_anom.append(host_galaxy_info[i])\n",
        "\n",
        "    else:\n",
        "        x_data.append(x[i])\n",
        "        y_data.append(target[i])\n",
        "        host_gal.append(host_galaxy_info[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "zwy3wjpPmFMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot Encoding\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "y_data = enc.fit_transform(np.array(y_data).reshape(-1, 1)).todense()"
      ],
      "metadata": {
        "id": "5Z2RcWxomLZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "\n",
        "X_train, X_test, host_gal_train, host_gal_test, y_train, y_test = train_test_split(x_data, host_gal, y_data, random_state = 40, test_size = 0.1)\n",
        "\n",
        "X_train, X_val, host_gal_train, host_gal_val, y_train, y_val = train_test_split(X_train, host_gal_train, y_train, random_state = 40, test_size = 0.125)"
      ],
      "metadata": {
        "id": "N5N3kmc5mOcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {i : 0 for i in range(y_train.shape[1])}\n",
        "\n",
        "for value in y_train:\n",
        "  class_weights[np.argmax(value)]+=1\n",
        "\n",
        "for id in class_weights.keys():\n",
        "  class_weights[id] = len(y_train) / class_weights[id]\n"
      ],
      "metadata": {
        "id": "Bisar4x4mRA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, LSTM, TimeDistributed, Dense, Masking, concatenate, GRU\n",
        "\n",
        "\n",
        "num_classes = len(class_weights)\n",
        "n_features = 4\n",
        "\n",
        "input_1 = Input((ntimesteps, n_features))  # X.shape = (Nobjects, Ntimesteps, 4)\n",
        "\n",
        "masking_input1 = Masking(mask_value=0.)(input_1)\n",
        "\n",
        "lstm1 = GRU(100, return_sequences=True, activation='tanh', recurrent_activation='hard_sigmoid')(masking_input1)\n",
        "lstm2 = GRU(100, return_sequences=False, activation='relu', recurrent_activation='hard_sigmoid')(lstm1)\n",
        "\n",
        "dense1 = Dense(100, activation='relu')(lstm2)\n",
        "\n",
        "input_2 = Input(shape = (len(host_galaxy_info[0]), ))\n",
        "\n",
        "dense2 = Dense(30)(input_2)\n",
        "\n",
        "merge1 = concatenate([dense1, dense2])\n",
        "\n",
        "dense3 = Dense(100, activation='relu')(merge1)\n",
        "\n",
        "dense4 = Dense(9, activation='relu')(dense3)\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(dense4)\n",
        "\n",
        "model = keras.Model(inputs=[input_1, input_2], outputs=output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "CwOkVsDUmWUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)\n",
        "host_gal_train = np.array(host_gal_train)\n",
        "host_gal_val = np.array(host_gal_val)"
      ],
      "metadata": {
        "id": "pl5eiVzYmXF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model.fit(x = [X_train, host_gal_train], y = y_train, epochs=100, batch_size = 128, class_weight = class_weights, validation_data=([X_val, host_gal_val], y_val))\n",
        "except KeyboardInterrupt:\n",
        "    save_path = \"trained_model\"\n",
        "    model.save(save_path)\n",
        "    print('Output saved to: \"{}./*\"'.format(save_path))"
      ],
      "metadata": {
        "id": "X_olHbOVmYc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}